{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb029409",
   "metadata": {
    "papermill": {
     "duration": 0.005579,
     "end_time": "2025-04-14T01:04:40.192314",
     "exception": false,
     "start_time": "2025-04-14T01:04:40.186735",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ProGAN\n",
    "\n",
    "[sauce](https://arxiv.org/pdf/1710.10196)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eb2afc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T01:04:40.203074Z",
     "iopub.status.busy": "2025-04-14T01:04:40.202702Z",
     "iopub.status.idle": "2025-04-14T01:04:44.500429Z",
     "shell.execute_reply": "2025-04-14T01:04:44.499492Z"
    },
    "papermill": {
     "duration": 4.304842,
     "end_time": "2025-04-14T01:04:44.501972",
     "exception": false,
     "start_time": "2025-04-14T01:04:40.197130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install -q onnx torchinfo torchmetrics[image]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551e02c8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-14T01:04:44.512589Z",
     "iopub.status.busy": "2025-04-14T01:04:44.512359Z",
     "iopub.status.idle": "2025-04-14T01:04:55.660602Z",
     "shell.execute_reply": "2025-04-14T01:04:55.659893Z"
    },
    "papermill": {
     "duration": 11.15522,
     "end_time": "2025-04-14T01:04:55.662260",
     "exception": false,
     "start_time": "2025-04-14T01:04:44.507040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchinfo import summary\n",
    "from torchmetrics.image.inception import InceptionScore\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.image.perceptual_path_length import PerceptualPathLength\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Any, Callable, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ab03d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T01:04:55.673211Z",
     "iopub.status.busy": "2025-04-14T01:04:55.672839Z",
     "iopub.status.idle": "2025-04-14T01:04:55.724094Z",
     "shell.execute_reply": "2025-04-14T01:04:55.723345Z"
    },
    "papermill": {
     "duration": 0.05798,
     "end_time": "2025-04-14T01:04:55.725360",
     "exception": false,
     "start_time": "2025-04-14T01:04:55.667380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# General\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "RANDOM_SEED = 420\n",
    "\n",
    "# Dataloader\n",
    "DATASET_PATH = '/kaggle/input/batik-dataset-for-gan/Dataset Final'\n",
    "BATCH_SIZES = [16, 16, 16, 8, 8, 4, 4]\n",
    "NUM_WORKERS = 4\n",
    "SHUFFLE = True\n",
    "PIN_MEMORY = False\n",
    "\n",
    "# Modelling\n",
    "LATENT_FEATURES = 512\n",
    "RESOLUTION = 128\n",
    "\n",
    "# Training\n",
    "LEARNING_RATE = 2e-3\n",
    "NUM_EPOCHS = 120\n",
    "N_CRITICS = 1\n",
    "GP_LAMBDA = 10\n",
    "EPS_DRIFT = 1e-3\n",
    "OUTPUT_DIR = 'generated_images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a75f53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T01:04:55.735637Z",
     "iopub.status.busy": "2025-04-14T01:04:55.735410Z",
     "iopub.status.idle": "2025-04-14T01:04:55.743888Z",
     "shell.execute_reply": "2025-04-14T01:04:55.743067Z"
    },
    "papermill": {
     "duration": 0.014941,
     "end_time": "2025-04-14T01:04:55.745164",
     "exception": false,
     "start_time": "2025-04-14T01:04:55.730223",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.deterministic = True\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "torch.autograd.profiler.profile(False)\n",
    "torch.autograd.profiler.emit_nvtx(False)\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdbadd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T01:04:55.755156Z",
     "iopub.status.busy": "2025-04-14T01:04:55.754948Z",
     "iopub.status.idle": "2025-04-14T01:04:55.760203Z",
     "shell.execute_reply": "2025-04-14T01:04:55.759627Z"
    },
    "papermill": {
     "duration": 0.01149,
     "end_time": "2025-04-14T01:04:55.761411",
     "exception": false,
     "start_time": "2025-04-14T01:04:55.749921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BatikGANDataset(Dataset):\n",
    "    '''\n",
    "    BatikGAN Dataset Implementation with lazy loading.\n",
    "    \n",
    "    Args:\n",
    "        path (str): Path to image directory.\n",
    "        transform (callable, optional): Image transforms that takes a PIL.Image as input. Default value is None.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, path: str, transform: Optional[Callable[Image.Image, Any]] = None):\n",
    "        super(BatikGANDataset, self).__init__()\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        self.files = [ f for f in os.listdir(self.path) if f.endswith(('.png', '.jpg', '.jpeg')) ]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, index: int) -> torch.Tensor:\n",
    "        img_path = os.path.join(self.path, self.files[index])\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9513999",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T01:04:55.771329Z",
     "iopub.status.busy": "2025-04-14T01:04:55.771106Z",
     "iopub.status.idle": "2025-04-14T01:04:55.775083Z",
     "shell.execute_reply": "2025-04-14T01:04:55.774435Z"
    },
    "papermill": {
     "duration": 0.010264,
     "end_time": "2025-04-14T01:04:55.776341",
     "exception": false,
     "start_time": "2025-04-14T01:04:55.766077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_loader(resolution: int) -> DataLoader:\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.Resize(resolution),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5] * 3, [0.5] * 3)\n",
    "    ])\n",
    "    dataset = BatikGANDataset(DATASET_PATH, transform=transform)\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZES[int(math.log2(resolution)) - 2], num_workers=NUM_WORKERS, shuffle=SHUFFLE, pin_memory=PIN_MEMORY)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7b914d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T01:04:55.786181Z",
     "iopub.status.busy": "2025-04-14T01:04:55.785979Z",
     "iopub.status.idle": "2025-04-14T01:04:56.651134Z",
     "shell.execute_reply": "2025-04-14T01:04:56.650212Z"
    },
    "papermill": {
     "duration": 0.879607,
     "end_time": "2025-04-14T01:04:56.660514",
     "exception": false,
     "start_time": "2025-04-14T01:04:55.780907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch = next(iter(get_loader(RESOLUTION)))\n",
    "grid = make_grid(batch, nrow=math.ceil(BATCH_SIZES[-1] ** .5), normalize=True)\n",
    "grid_np = grid.numpy().transpose((1, 2, 0))\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(grid_np)\n",
    "plt.axis('off')\n",
    "plt.title('Batch of Images from Batik GAN Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e843e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T01:04:56.689557Z",
     "iopub.status.busy": "2025-04-14T01:04:56.689291Z",
     "iopub.status.idle": "2025-04-14T01:04:56.694642Z",
     "shell.execute_reply": "2025-04-14T01:04:56.694025Z"
    },
    "papermill": {
     "duration": 0.021533,
     "end_time": "2025-04-14T01:04:56.695969",
     "exception": false,
     "start_time": "2025-04-14T01:04:56.674436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WSConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Weight-scaled Conv2d layer for equalized learning rate.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input channels.\n",
    "        out_channels (int): Number of output channels.\n",
    "        kernel_size (int, optional): Size of the convolving kernel. Default: 3.\n",
    "        stride (int, optional): Stride of the convolution. Default: 1.\n",
    "        padding (int, optional): Padding added to all sides of the input. Default: 1.\n",
    "        gain (float, optional): Gain factor for weight initialization. Default: 2.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, gain=2):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.scale = (gain / (in_channels * kernel_size ** 2)) ** 0.5\n",
    "        self.bias = self.conv.bias\n",
    "        self.conv.bias = None  # Remove bias to apply it after scaling\n",
    "\n",
    "        # Initialize weights\n",
    "        nn.init.normal_(self.conv.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79926b14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T01:04:56.723596Z",
     "iopub.status.busy": "2025-04-14T01:04:56.723368Z",
     "iopub.status.idle": "2025-04-14T01:04:56.728215Z",
     "shell.execute_reply": "2025-04-14T01:04:56.727574Z"
    },
    "papermill": {
     "duration": 0.019979,
     "end_time": "2025-04-14T01:04:56.729383",
     "exception": false,
     "start_time": "2025-04-14T01:04:56.709404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WSConvTranspose2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Weight-scaled ConvTranspose2d layer for equalized learning rate.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input channels.\n",
    "        out_channels (int): Number of output channels.\n",
    "        kernel_size (int, optional): Size of the convolving kernel. Default: 3.\n",
    "        stride (int, optional): Stride of the convolution. Default: 1.\n",
    "        padding (int, optional): Padding added to all sides of the input. Default: 1.\n",
    "        gain (float, optional): Gain factor for weight initialization. Default: 2.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, gain=2):\n",
    "        super().__init__()\n",
    "        self.conv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.scale = (gain / (in_channels * kernel_size ** 2)) ** 0.5\n",
    "        self.bias = self.conv.bias\n",
    "        self.conv.bias = None  # Remove bias to apply it after scaling\n",
    "\n",
    "        # Initialize weights\n",
    "        nn.init.normal_(self.conv.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32841481",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T01:04:56.757124Z",
     "iopub.status.busy": "2025-04-14T01:04:56.756902Z",
     "iopub.status.idle": "2025-04-14T01:04:56.760904Z",
     "shell.execute_reply": "2025-04-14T01:04:56.760302Z"
    },
    "papermill": {
     "duration": 0.01913,
     "end_time": "2025-04-14T01:04:56.762047",
     "exception": false,
     "start_time": "2025-04-14T01:04:56.742917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PixelNorm(nn.Module):\n",
    "    '''\n",
    "    Pixelwise Normalization.\n",
    "    \n",
    "    Args:\n",
    "        eps (float, optional): Small value to avoid division by zero. Default value is 1e-8.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, eps: float = 1e-8) -> None:\n",
    "        super(PixelNorm, self).__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (b, c, h, w).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor with the same shape as input.\n",
    "        '''\n",
    "        return x * torch.rsqrt(x.pow(2).mean(dim=1, keepdim=True) + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8be1ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T01:04:56.789722Z",
     "iopub.status.busy": "2025-04-14T01:04:56.789502Z",
     "iopub.status.idle": "2025-04-14T01:04:56.793423Z",
     "shell.execute_reply": "2025-04-14T01:04:56.792807Z"
    },
    "papermill": {
     "duration": 0.019103,
     "end_time": "2025-04-14T01:04:56.794555",
     "exception": false,
     "start_time": "2025-04-14T01:04:56.775452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CPL(nn.Sequential):\n",
    "    '''\n",
    "    A sequential layer consisting of:\n",
    "    WSConv2d -> PixelNorm -> LeakyReLU\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): The number of input channels.\n",
    "        out_channels (int): The number of output channels.\n",
    "        norm (bool, optional): Enable PixelNorm. Default value is True.\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_channels: int, \n",
    "        out_channels: int, \n",
    "        norm: bool = True, \n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super(CPL, self).__init__(\n",
    "            WSConv2d(in_channels, out_channels, **kwargs),\n",
    "            PixelNorm() if norm else nn.Identity(),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c98856f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T01:04:56.822137Z",
     "iopub.status.busy": "2025-04-14T01:04:56.821937Z",
     "iopub.status.idle": "2025-04-14T01:04:56.825162Z",
     "shell.execute_reply": "2025-04-14T01:04:56.824578Z"
    },
    "papermill": {
     "duration": 0.018219,
     "end_time": "2025-04-14T01:04:56.826227",
     "exception": false,
     "start_time": "2025-04-14T01:04:56.808008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Sequential):\n",
    "    \"\"\"\n",
    "    Convolutional block with two CPL layers.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input channels.\n",
    "        out_channels (int): Number of output channels.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__(\n",
    "            CPL(in_channels, out_channels),\n",
    "            CPL(out_channels, out_channels)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031f6d4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T01:04:56.854495Z",
     "iopub.status.busy": "2025-04-14T01:04:56.854270Z",
     "iopub.status.idle": "2025-04-14T01:04:56.862375Z",
     "shell.execute_reply": "2025-04-14T01:04:56.861768Z"
    },
    "papermill": {
     "duration": 0.023641,
     "end_time": "2025-04-14T01:04:56.863475",
     "exception": false,
     "start_time": "2025-04-14T01:04:56.839834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    ProGAN Generator\n",
    "\n",
    "    Args:\n",
    "        resolution (int): The resolution of the image.\n",
    "        latent_dim (int): The number of channels in the latent vector.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, resolution: int, latent_dim: int) -> None:\n",
    "        super(Generator, self).__init__()\n",
    "        self.resolution = resolution\n",
    "        self.latent_dim = latent_dim\n",
    "        self.resolution_levels = int(math.log2(resolution)) - 1\n",
    "\n",
    "        self.blocks = nn.ModuleList([ nn.Sequential(\n",
    "            WSConvTranspose2d(self.latent_dim, self.latent_dim, kernel_size=4, padding=0),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            PixelNorm(),\n",
    "            CPL(self.latent_dim, self.latent_dim)\n",
    "        ) ])\n",
    "        self.to_rgb = nn.ModuleList([ WSConv2d(self.latent_dim, 3, kernel_size=1, padding=0) ])\n",
    "\n",
    "        for _ in range(3):\n",
    "            self.blocks.append(ConvBlock(self.latent_dim, self.latent_dim))\n",
    "            self.to_rgb.append(WSConv2d(self.latent_dim, 3, kernel_size=1, padding=0))\n",
    "\n",
    "        in_channels = self.latent_dim\n",
    "\n",
    "        for _ in range(self.resolution_levels - 4):\n",
    "            self.blocks.append(ConvBlock(in_channels, in_channels // 2))\n",
    "            self.to_rgb.append(WSConv2d(in_channels // 2, 3, kernel_size=1, padding=0))\n",
    "            in_channels //= 2\n",
    "\n",
    "    def forward(self, x: torch.Tensor, alpha: float = 1, steps: int = None) -> torch.Tensor:\n",
    "        '''\n",
    "        Args:\n",
    "            x (torch.Tensor): Input latent vector tensor of shape (b, l)\n",
    "            alpha (float, optional): Fade in alpha value. Default value is 1.\n",
    "            steps (int, optional): The number of steps starting from 0. If None, then the maximum number of steps is used. Default value is None.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (b, 3, h, w).\n",
    "        '''\n",
    "\n",
    "        if steps is None:\n",
    "            steps = self.resolution_levels - 1\n",
    "\n",
    "        x = self.blocks[0](x) # (b, 3, 4, 4)\n",
    "        \n",
    "        if steps == 0:\n",
    "            x = self.to_rgb[0](x) # (b, 3, 4, 4)\n",
    "            return x\n",
    "        \n",
    "        for i in range(1, steps):\n",
    "            x = F.interpolate(x, scale_factor=2, mode='nearest') # (b, 3, h/2, w/2)\n",
    "            x = self.blocks[i](x) # (b, 3, h/2, w/2)\n",
    "\n",
    "        if alpha < 1:\n",
    "            old_rgb = self.to_rgb[steps - 1](x) # (b, 3, h/2, w/2)\n",
    "            old_rgb = F.interpolate(old_rgb, scale_factor=2, mode='nearest') # (b, 3, h, w)\n",
    "        \n",
    "        x = F.interpolate(x, scale_factor=2, mode='nearest') # (b, 3, h, w)\n",
    "        x = self.blocks[steps](x) # (b, 3, h, w)\n",
    "        new_rgb = self.to_rgb[steps](x) # (b, 3, h, w)\n",
    "\n",
    "        if alpha < 1:\n",
    "            x = (1 - alpha) * old_rgb + alpha * new_rgb # (b, 3, h, w)\n",
    "        else:\n",
    "            x = new_rgb # (b, 3, h, w)\n",
    "        \n",
    "        x = x.tanh() # (b, 3, h, w)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77a06ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T01:04:56.891545Z",
     "iopub.status.busy": "2025-04-14T01:04:56.891323Z",
     "iopub.status.idle": "2025-04-14T01:04:57.621542Z",
     "shell.execute_reply": "2025-04-14T01:04:57.620595Z"
    },
    "papermill": {
     "duration": 0.745889,
     "end_time": "2025-04-14T01:04:57.623035",
     "exception": false,
     "start_time": "2025-04-14T01:04:56.877146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator_test = Generator(RESOLUTION, LATENT_FEATURES)\n",
    "summary(generator_test, input_data=[torch.randn(1, LATENT_FEATURES, 1, 1), 1.0, int(math.log2(RESOLUTION)) - 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc03bd01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T01:04:57.657097Z",
     "iopub.status.busy": "2025-04-14T01:04:57.656823Z",
     "iopub.status.idle": "2025-04-14T01:04:57.667037Z",
     "shell.execute_reply": "2025-04-14T01:04:57.666368Z"
    },
    "papermill": {
     "duration": 0.0281,
     "end_time": "2025-04-14T01:04:57.668285",
     "exception": false,
     "start_time": "2025-04-14T01:04:57.640185",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ProGANDiscriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Progressive GAN Discriminator compatible with StyleGAN generator.\n",
    "\n",
    "    Args:\n",
    "        resolution (int): Target resolution of the images.\n",
    "        features (int): Base number of features/channels.\n",
    "        img_channels (int, optional): Number of image channels. Default: 3.\n",
    "    \"\"\"\n",
    "    def __init__(self, resolution, features=512, img_channels=3):\n",
    "        super(ProGANDiscriminator, self).__init__()\n",
    "        self.resolution = resolution\n",
    "        self.features = features\n",
    "        self.img_channels = img_channels\n",
    "        self.resolution_levels = int(math.log2(resolution) - 1)\n",
    "\n",
    "        # Create progressive blocks and RGB layers\n",
    "        self.prog_blocks, self.rgb_layers = nn.ModuleList([]), nn.ModuleList([])\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "\n",
    "        # Calculate channel dimensions for each resolution level\n",
    "        # Start with minimum channels at highest resolution, double when going down\n",
    "        current_channels = features // (2 ** (self.resolution_levels - 4))\n",
    "        for i in range(self.resolution_levels, 0, -1):\n",
    "            # For first 4 resolution levels, keep channel count the same as base features\n",
    "            next_channels = features if i <= 4 else current_channels * 2\n",
    "\n",
    "            # Add conv block for this resolution level\n",
    "            self.prog_blocks.append(ConvBlock(current_channels, next_channels))\n",
    "            # Add RGB conversion layer for this resolution level\n",
    "            self.rgb_layers.append(\n",
    "                WSConv2d(img_channels, current_channels, kernel_size=1, stride=1, padding=0)\n",
    "            )\n",
    "\n",
    "            current_channels = next_channels\n",
    "\n",
    "        # For 4x4 resolution\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)  # Downsampling\n",
    "\n",
    "        # Final block for 4x4 resolution with minibatch std\n",
    "        self.final_block = nn.Sequential(\n",
    "            WSConv2d(features + 1, features, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(features, features, kernel_size=4, padding=0, stride=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(features, 1, kernel_size=1, padding=0, stride=1),\n",
    "        )\n",
    "\n",
    "    def minibatch_std(self, x):\n",
    "        \"\"\"Add minibatch standard deviation channel to the feature maps.\"\"\"\n",
    "        batch_statistics = torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])\n",
    "        return torch.cat([x, batch_statistics], dim=1)\n",
    "\n",
    "    def forward(self, x, alpha=1.0, current_step=0):\n",
    "        \"\"\"\n",
    "        Forward pass through the discriminator.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input images of shape (b, 3, h, w).\n",
    "            alpha (float): Fade-in factor for progressive growing [0, 1].\n",
    "            current_step (int): Current step in progressive growing (0 = lowest resolution).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Discriminator output of shape (b, 1).\n",
    "        \"\"\"\n",
    "        # Determine which layers to use based on the current step\n",
    "        step_index = self.resolution_levels - current_step - 1\n",
    "\n",
    "        # Process the input through the appropriate RGB layer\n",
    "        out = self.leaky(self.rgb_layers[step_index](x))\n",
    "\n",
    "        # If we're at 4x4 resolution (final step)\n",
    "        if step_index == self.resolution_levels - 1:\n",
    "            out = self.minibatch_std(out)\n",
    "            return self.final_block(out).view(out.shape[0], -1)\n",
    "\n",
    "        # Process through appropriate conv block\n",
    "        out = self.prog_blocks[step_index](out)\n",
    "        out = self.avg_pool(out)  # Downsample\n",
    "\n",
    "        # Handle fade-in if alpha < 1\n",
    "        if alpha < 1:\n",
    "            # Process the downsampled input through the next RGB layer\n",
    "            downscaled = self.avg_pool(x)\n",
    "            y = self.leaky(self.rgb_layers[step_index + 1](downscaled))\n",
    "            out = alpha * out + (1 - alpha) * y\n",
    "\n",
    "        # Continue processing through the remaining blocks\n",
    "        for i in range(step_index + 1, self.resolution_levels):\n",
    "            out = self.prog_blocks[i](out)\n",
    "            if i < self.resolution_levels - 1:  # Don't downsample at the final resolution\n",
    "                out = self.avg_pool(out)\n",
    "\n",
    "        # Final processing at 4x4 resolution\n",
    "        out = self.minibatch_std(out)\n",
    "        return self.final_block(out).view(out.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45de4a72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T01:04:57.699424Z",
     "iopub.status.busy": "2025-04-14T01:04:57.699159Z",
     "iopub.status.idle": "2025-04-14T01:04:59.104282Z",
     "shell.execute_reply": "2025-04-14T01:04:59.103327Z"
    },
    "papermill": {
     "duration": 1.422366,
     "end_time": "2025-04-14T01:04:59.105806",
     "exception": false,
     "start_time": "2025-04-14T01:04:57.683440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "discriminator_test = ProGANDiscriminator(RESOLUTION, LATENT_FEATURES)\n",
    "summary(discriminator_test, input_data=[torch.randn(2, 3, RESOLUTION, RESOLUTION), 1.0, int(math.log2(RESOLUTION)) - 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e432f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T01:04:59.139345Z",
     "iopub.status.busy": "2025-04-14T01:04:59.139061Z",
     "iopub.status.idle": "2025-04-14T01:05:00.109806Z",
     "shell.execute_reply": "2025-04-14T01:05:00.108832Z"
    },
    "papermill": {
     "duration": 0.988901,
     "end_time": "2025-04-14T01:05:00.111489",
     "exception": false,
     "start_time": "2025-04-14T01:04:59.122588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator = Generator(RESOLUTION, LATENT_FEATURES).to(DEVICE)\n",
    "discriminator = ProGANDiscriminator(RESOLUTION, 512).to(DEVICE)\n",
    "\n",
    "optim_g = optim.Adam(generator.parameters(), lr=LEARNING_RATE, betas=(0, .99))\n",
    "optim_d = optim.Adam(discriminator.parameters(), lr=LEARNING_RATE, betas=(0, .99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaeed58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T01:05:00.144010Z",
     "iopub.status.busy": "2025-04-14T01:05:00.143695Z",
     "iopub.status.idle": "2025-04-14T01:05:00.150524Z",
     "shell.execute_reply": "2025-04-14T01:05:00.149888Z"
    },
    "papermill": {
     "duration": 0.024149,
     "end_time": "2025-04-14T01:05:00.151701",
     "exception": false,
     "start_time": "2025-04-14T01:05:00.127552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(\n",
    "    generator: nn.Module,\n",
    "    discriminator: nn.Module,\n",
    "    optim_g: optim.Optimizer,\n",
    "    optim_d: optim.Optimizer,\n",
    "    epoch: int,\n",
    "    *,\n",
    "    resolution: int, \n",
    "    alpha: float, \n",
    "    step: int, \n",
    "    fname: str = 'result.png', \n",
    "    rows: int = 4,\n",
    "    last: bool = True\n",
    ") -> None:\n",
    "    '''\n",
    "    Saves checkpoint and saves generated images from the checkpoint state.\n",
    "\n",
    "    Args:\n",
    "        generator (nn.Module): The generator network.\n",
    "        discriminator (nn.Module): The discriminator network.\n",
    "        optim_g (nn.Module): The generator's optimizer.\n",
    "        optim_d (nn.Module): The discriminator's optimizer.\n",
    "        epoch (int): The epoch of the checkpoint.\n",
    "        resolution (int): The current resolution of the network.\n",
    "        alpha (float): Fade-in alpha value.\n",
    "        step (int): The number of steps taken by the network.\n",
    "        fname (str, optional): The generated images file name. Default value is 'result.png'.\n",
    "        rows (int, optional): The number of rows of the generated images grid. Default value is 4.\n",
    "        last (bool, optional): Save as most recent checkpoint. Default value is True.\n",
    "    '''\n",
    "    \n",
    "    checkpoint = {\n",
    "        'generator': generator.state_dict(),\n",
    "        'discriminator': discriminator.state_dict(),\n",
    "        'optim_g': optim_g.state_dict(),\n",
    "        'optim_d': optim_d.state_dict(),\n",
    "        'resolution': resolution,\n",
    "        'epoch': epoch,\n",
    "        'alpha': alpha,\n",
    "        'step': step,\n",
    "    }\n",
    "    \n",
    "    epoch_path = os.path.join(f'epoch{epoch}')\n",
    "    os.makedirs(epoch_path, exist_ok=True)\n",
    "    torch.save(checkpoint, os.path.join(epoch_path, 'checkpoint.pt'))\n",
    "    \n",
    "    generator.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(rows * rows, LATENT_FEATURES, 1, 1, device=DEVICE)\n",
    "        outputs = generator(z, alpha, step)\n",
    "        outputs = F.interpolate(outputs, size=(RESOLUTION, RESOLUTION), mode='bilinear', align_corners=True)\n",
    "        \n",
    "        outputs = make_grid(outputs, nrow=rows, normalize=True)\n",
    "        save_image(outputs, os.path.join(epoch_path, fname))\n",
    "        \n",
    "        if last:\n",
    "            last_path = os.path.join('last')\n",
    "            os.makedirs(last_path, exist_ok=True)\n",
    "            torch.save(checkpoint, os.path.join(last_path, 'checkpoint.pt'))\n",
    "            save_image(outputs, os.path.join(last_path, fname))\n",
    "\n",
    "    generator.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc7fe23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T01:05:00.183514Z",
     "iopub.status.busy": "2025-04-14T01:05:00.183289Z",
     "iopub.status.idle": "2025-04-14T01:05:00.196968Z",
     "shell.execute_reply": "2025-04-14T01:05:00.196339Z"
    },
    "papermill": {
     "duration": 0.031217,
     "end_time": "2025-04-14T01:05:00.198144",
     "exception": false,
     "start_time": "2025-04-14T01:05:00.166927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(checkpoint: str = None):\n",
    "    losses_d = []\n",
    "    losses_g = []\n",
    "    \n",
    "    # Menentukan awal epoch, step dan alpha\n",
    "    start = 1\n",
    "    step = 0\n",
    "    alpha = 1e-5\n",
    "    \n",
    "    # Mulai dari resolusi terkecil (biasanya 4x4)\n",
    "    current_resolution = 4\n",
    "\n",
    "    if checkpoint is not None and os.path.exists(checkpoint):\n",
    "        print('Resuming from last checkpoint...\\n')\n",
    "        last_checkpoint = torch.load(os.path.join(checkpoint), weights_only=True, map_location=DEVICE)\n",
    "        generator.load_state_dict(last_checkpoint['generator'])\n",
    "        discriminator.load_state_dict(last_checkpoint['discriminator'])\n",
    "        optim_d.load_state_dict(last_checkpoint['optim_d'])\n",
    "        optim_g.load_state_dict(last_checkpoint['optim_g'])\n",
    "        current_resolution = last_checkpoint['resolution']\n",
    "        start = last_checkpoint['epoch'] + 1\n",
    "        alpha = last_checkpoint['alpha']\n",
    "        step = last_checkpoint['step']\n",
    "    \n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "\n",
    "    # Dapatkan loader untuk resolusi awal\n",
    "    loader = get_loader(current_resolution)\n",
    "\n",
    "    # Parameter untuk peningkatan progresif\n",
    "    steps_per_resolution = len(loader) * (NUM_EPOCHS // (int(math.log2(RESOLUTION)) - 1))\n",
    "    alpha_step = 1.0 / steps_per_resolution\n",
    "    \n",
    "    for epoch in range(start, NUM_EPOCHS + 1):\n",
    "        print(f'[Epoch {epoch} / {NUM_EPOCHS}]')\n",
    "        avg_g_loss = 0\n",
    "        avg_d_loss = 0\n",
    "\n",
    "        # Periksa apakah harus menaikkan resolusi\n",
    "        if epoch % (NUM_EPOCHS // (int(math.log2(RESOLUTION)) - 1)) == 0 and current_resolution < RESOLUTION:\n",
    "            current_resolution *= 2\n",
    "            step += 1\n",
    "            alpha = 1e-5\n",
    "\n",
    "            # Atur ulang dataloader untuk resolusi baru\n",
    "            loader = get_loader(current_resolution)\n",
    "            steps_per_resolution = len(loader) * (NUM_EPOCHS // (int(math.log2(RESOLUTION)) - 1))\n",
    "            alpha_step = 1.0 / steps_per_resolution\n",
    "\n",
    "            print(f\"Resolution increased to {current_resolution}x{current_resolution}\")\n",
    "\n",
    "        progress_bar = tqdm(total=len(loader), \n",
    "                        desc=f'Train - Resolution: {current_resolution}x{current_resolution}', \n",
    "                        unit='step')\n",
    "        progress_bar.set_postfix_str(f'Alpha: {alpha:.4f}')\n",
    "\n",
    "        for real_img in loader:\n",
    "            b = real_img.size(0)\n",
    "            real_img = real_img.to(DEVICE)\n",
    "            \n",
    "            # Meningkatkan alpha secara bertahap\n",
    "            if alpha < 1.0:\n",
    "                alpha += alpha_step\n",
    "                alpha = min(alpha, 1)\n",
    "                progress_bar.set_postfix_str(f'Alpha: {alpha:.4f}')\n",
    "                \n",
    "            # Discriminator\n",
    "            for p in discriminator.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "            for _ in range(N_CRITICS):\n",
    "                optim_d.zero_grad()\n",
    "        \n",
    "                # Menghasilkan noise latent untuk generator\n",
    "                z = torch.randn(b, LATENT_FEATURES, 1, 1, device=DEVICE)\n",
    "        \n",
    "                # Menghasilkan gambar palsu dengan generator\n",
    "                fake_img = generator(z, alpha, step)\n",
    "        \n",
    "                # Forward pass pada discriminator dengan parameter progresif\n",
    "                real_logits = discriminator(real_img, alpha, step)\n",
    "                fake_logits = discriminator(fake_img.detach(), alpha, step)\n",
    "    \n",
    "                # Gradient penalty\n",
    "                eps = torch.rand(b, 1, 1, 1, device=DEVICE, requires_grad=True)\n",
    "                interpolated = eps * real_img + (1 - eps) * fake_img.detach()\n",
    "                interpolated_logits = discriminator(interpolated, alpha, step)\n",
    "                grad = torch.autograd.grad(\n",
    "                    outputs=interpolated_logits, \n",
    "                    inputs=interpolated, \n",
    "                    grad_outputs=torch.ones_like(interpolated_logits),\n",
    "                    create_graph=True,\n",
    "                    retain_graph=True\n",
    "                )[0]\n",
    "                grad = grad.view(grad.size(0), -1)\n",
    "                grad_penalty = (grad.norm(2, dim=1) - 1).pow(2).mean()\n",
    "\n",
    "                # Drift penalty\n",
    "                drift_penalty = real_logits.pow(2).mean()\n",
    "    \n",
    "                # Total discriminator loss\n",
    "                d_loss = (\n",
    "                    fake_logits.mean() - real_logits.mean() \n",
    "                    + GP_LAMBDA * grad_penalty \n",
    "                    + EPS_DRIFT * drift_penalty\n",
    "                )\n",
    "                avg_d_loss += d_loss.item()\n",
    "    \n",
    "                d_loss.backward()\n",
    "                optim_d.step()\n",
    "\n",
    "            # Generator\n",
    "            for p in discriminator.parameters():\n",
    "                p.requires_grad = False\n",
    "            \n",
    "            optim_g.zero_grad()\n",
    "\n",
    "            # Buat gambar palsu baru dan evaluasi\n",
    "            z = torch.randn(b, LATENT_FEATURES, 1, 1, device=DEVICE)\n",
    "            fake_img = generator(z, alpha, step)\n",
    "            fake_logits = discriminator(fake_img, alpha, step)\n",
    "            g_loss = -fake_logits.mean()\n",
    "            avg_g_loss += g_loss.item()\n",
    "\n",
    "            g_loss.backward()\n",
    "            optim_g.step()\n",
    "            \n",
    "            progress_bar.update(1)\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        avg_d_loss /= len(loader) * N_CRITICS\n",
    "        avg_g_loss /= len(loader)\n",
    "        print(f'Generator: {avg_g_loss:.4f}, Discriminator: {avg_d_loss:.4f}, Resolution: {current_resolution}x{current_resolution}, Alpha: {alpha:.4f}\\n')\n",
    "        \n",
    "        losses_d.append(avg_d_loss)\n",
    "        losses_g.append(avg_g_loss)\n",
    "\n",
    "        if epoch % 5 != 0:\n",
    "            continue\n",
    "\n",
    "        save_checkpoint(\n",
    "            generator,\n",
    "            discriminator,\n",
    "            optim_g,\n",
    "            optim_d,\n",
    "            epoch=epoch,\n",
    "            resolution=current_resolution, \n",
    "            alpha=alpha, \n",
    "            step=step, \n",
    "        )\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses_g, label='Generator')\n",
    "    plt.plot(losses_d, label='Discriminator')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Losses')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('loss_plot.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2345d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T01:05:00.229265Z",
     "iopub.status.busy": "2025-04-14T01:05:00.229016Z",
     "iopub.status.idle": "2025-04-14T01:05:00.232229Z",
     "shell.execute_reply": "2025-04-14T01:05:00.231581Z"
    },
    "papermill": {
     "duration": 0.019956,
     "end_time": "2025-04-14T01:05:00.233424",
     "exception": false,
     "start_time": "2025-04-14T01:05:00.213468",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1e0a69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T01:05:00.264350Z",
     "iopub.status.busy": "2025-04-14T01:05:00.264137Z",
     "iopub.status.idle": "2025-04-14T01:05:04.389120Z",
     "shell.execute_reply": "2025-04-14T01:05:04.388253Z"
    },
    "papermill": {
     "duration": 4.14181,
     "end_time": "2025-04-14T01:05:04.390487",
     "exception": false,
     "start_time": "2025-04-14T01:05:00.248677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load(os.path.join('last', 'checkpoint.pt'), weights_only=True, map_location=DEVICE)\n",
    "\n",
    "generator.load_state_dict(checkpoint['generator'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a38333c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T01:05:04.423060Z",
     "iopub.status.busy": "2025-04-14T01:05:04.422805Z",
     "iopub.status.idle": "2025-04-14T01:05:10.886115Z",
     "shell.execute_reply": "2025-04-14T01:05:10.885154Z"
    },
    "papermill": {
     "duration": 6.497221,
     "end_time": "2025-04-14T01:05:10.904108",
     "exception": false,
     "start_time": "2025-04-14T01:05:04.406887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROWS = 8\n",
    "\n",
    "generator.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(ROWS * ROWS, LATENT_FEATURES, 1, 1, device=DEVICE)\n",
    "    imgs = generator(z)\n",
    "    \n",
    "    grid = make_grid(imgs, nrow=ROWS, normalize=True)\n",
    "    grid_np = grid.cpu().numpy().transpose((1, 2, 0))\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(grid_np)\n",
    "    plt.axis('off')\n",
    "    plt.title('Batik-ProGAN Results')\n",
    "    plt.savefig('final_results.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57b0efc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T01:05:10.982530Z",
     "iopub.status.busy": "2025-04-14T01:05:10.982233Z",
     "iopub.status.idle": "2025-04-14T01:05:17.181686Z",
     "shell.execute_reply": "2025-04-14T01:05:17.180565Z"
    },
    "papermill": {
     "duration": 6.239631,
     "end_time": "2025-04-14T01:05:17.183126",
     "exception": false,
     "start_time": "2025-04-14T01:05:10.943495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator.eval()\n",
    "\n",
    "inception_score = InceptionScore(normalize=True).to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(32, LATENT_FEATURES, 1, 1, device=DEVICE)\n",
    "    images = generator(z)\n",
    "    images = images * 0.5 + 0.5\n",
    "    images = F.interpolate(images, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "    inception_score.update(images)\n",
    "\n",
    "inception_mean, inception_std = inception_score.compute()\n",
    "\n",
    "print(f'IS: {inception_mean.item()} +/- {inception_std.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f41380",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T01:05:17.257929Z",
     "iopub.status.busy": "2025-04-14T01:05:17.257590Z",
     "iopub.status.idle": "2025-04-14T01:06:29.166437Z",
     "shell.execute_reply": "2025-04-14T01:06:29.165323Z"
    },
    "papermill": {
     "duration": 71.984568,
     "end_time": "2025-04-14T01:06:29.204439",
     "exception": false,
     "start_time": "2025-04-14T01:05:17.219871",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator.eval()\n",
    "\n",
    "loader = get_loader(RESOLUTION)\n",
    "fid = FrechetInceptionDistance(normalize=True).to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for real_img in loader:\n",
    "        real_img = real_img.to(DEVICE)\n",
    "        b = real_img.size(0)\n",
    "    \n",
    "        z = torch.randn(b, LATENT_FEATURES, 1, 1, device=DEVICE)\n",
    "        fake_img = generator(z)\n",
    "    \n",
    "        real_img = real_img * 0.5 + 0.5\n",
    "        fake_img = fake_img * 0.5 + 0.5\n",
    "    \n",
    "        real_img = F.interpolate(real_img, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "        fake_img = F.interpolate(fake_img, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "    \n",
    "        fid.update(real_img, real=True)\n",
    "        fid.update(fake_img, real=False)\n",
    "\n",
    "fid_score = fid.compute()\n",
    "\n",
    "print(f'FID: {fid_score.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a7d1f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T01:06:29.278059Z",
     "iopub.status.busy": "2025-04-14T01:06:29.277702Z",
     "iopub.status.idle": "2025-04-14T01:07:45.058850Z",
     "shell.execute_reply": "2025-04-14T01:07:45.057765Z"
    },
    "papermill": {
     "duration": 75.858441,
     "end_time": "2025-04-14T01:07:45.098978",
     "exception": false,
     "start_time": "2025-04-14T01:06:29.240537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PPLWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super(PPLWrapper, self).__init__()\n",
    "\n",
    "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            images = generator(z)\n",
    "        images = images * 0.5 + 0.5\n",
    "        images *= 255\n",
    "        images = images.to(torch.uint8)\n",
    "        return images\n",
    "\n",
    "    def sample(self, num_samples: int) -> torch.Tensor:\n",
    "        return torch.randn(num_samples, LATENT_FEATURES, 1, 1, device=DEVICE)\n",
    "\n",
    "generator.eval()\n",
    "\n",
    "ppl = PerceptualPathLength().to(DEVICE)\n",
    "\n",
    "ppl_mean, ppl_std, ppl_raw = ppl(PPLWrapper())\n",
    "\n",
    "print(f'PPL: {ppl_mean.item()} +/- {ppl_std.item()}, Raw: {ppl_raw}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980ff011",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T01:07:45.175796Z",
     "iopub.status.busy": "2025-04-14T01:07:45.175447Z",
     "iopub.status.idle": "2025-04-14T01:07:47.361503Z",
     "shell.execute_reply": "2025-04-14T01:07:47.360750Z"
    },
    "papermill": {
     "duration": 2.226738,
     "end_time": "2025-04-14T01:07:47.363193",
     "exception": false,
     "start_time": "2025-04-14T01:07:45.136455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator.eval()\n",
    "\n",
    "z = torch.randn(1, LATENT_FEATURES, 1, 1, device=DEVICE)\n",
    "\n",
    "torch.onnx.export(\n",
    "    generator,\n",
    "    (z,),\n",
    "    'batik_progan.onnx',\n",
    "    input_names=['z'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={'z': {0: 'batch_size'}},\n",
    "    opset_version=16,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6987767,
     "sourceId": 11193215,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 294234,
     "modelInstanceId": 281165,
     "sourceId": 335910,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 294234,
     "modelInstanceId": 281165,
     "sourceId": 336373,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 192.846367,
   "end_time": "2025-04-14T01:07:50.481623",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-14T01:04:37.635256",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
