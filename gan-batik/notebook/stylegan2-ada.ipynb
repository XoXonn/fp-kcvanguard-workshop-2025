{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-04T18:23:58.316446Z",
     "iopub.status.busy": "2025-04-04T18:23:58.315898Z",
     "iopub.status.idle": "2025-04-04T18:23:59.948235Z",
     "shell.execute_reply": "2025-04-04T18:23:59.948693Z",
     "shell.execute_reply.started": "2025-04-04T14:22:59.545548Z"
    },
    "papermill": {
     "duration": 1.644612,
     "end_time": "2025-04-04T18:23:59.948970",
     "exception": false,
     "start_time": "2025-04-04T18:23:58.304358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T18:23:59.968392Z",
     "iopub.status.busy": "2025-04-04T18:23:59.967796Z",
     "iopub.status.idle": "2025-04-04T18:23:59.970564Z",
     "shell.execute_reply": "2025-04-04T18:23:59.970934Z",
     "shell.execute_reply.started": "2025-04-04T14:23:00.988618Z"
    },
    "papermill": {
     "duration": 0.014711,
     "end_time": "2025-04-04T18:23:59.971087",
     "exception": false,
     "start_time": "2025-04-04T18:23:59.956376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd stylegan2-ada-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T18:23:59.992197Z",
     "iopub.status.busy": "2025-04-04T18:23:59.987470Z",
     "iopub.status.idle": "2025-04-04T18:24:01.033987Z",
     "shell.execute_reply": "2025-04-04T18:24:01.033466Z",
     "shell.execute_reply.started": "2025-04-04T14:23:02.430898Z"
    },
    "papermill": {
     "duration": 1.056199,
     "end_time": "2025-04-04T18:24:01.034155",
     "exception": false,
     "start_time": "2025-04-04T18:23:59.977956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T18:24:01.060747Z",
     "iopub.status.busy": "2025-04-04T18:24:01.059710Z",
     "iopub.status.idle": "2025-04-04T18:24:02.049595Z",
     "shell.execute_reply": "2025-04-04T18:24:02.049136Z",
     "shell.execute_reply.started": "2025-04-04T14:23:03.453513Z"
    },
    "papermill": {
     "duration": 1.006871,
     "end_time": "2025-04-04T18:24:02.049730",
     "exception": false,
     "start_time": "2025-04-04T18:24:01.042859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T18:24:02.073333Z",
     "iopub.status.busy": "2025-04-04T18:24:02.068515Z",
     "iopub.status.idle": "2025-04-04T18:24:33.661991Z",
     "shell.execute_reply": "2025-04-04T18:24:33.661547Z",
     "shell.execute_reply.started": "2025-04-04T14:23:04.467897Z"
    },
    "papermill": {
     "duration": 31.605105,
     "end_time": "2025-04-04T18:24:33.662124",
     "exception": false,
     "start_time": "2025-04-04T18:24:02.057019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install click requests tqdm pyspng ninja imageio-ffmpeg==0.4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T18:24:33.717919Z",
     "iopub.status.busy": "2025-04-04T18:24:33.716867Z",
     "iopub.status.idle": "2025-04-04T18:24:34.904269Z",
     "shell.execute_reply": "2025-04-04T18:24:34.903778Z",
     "shell.execute_reply.started": "2025-04-04T14:23:09.671683Z"
    },
    "papermill": {
     "duration": 1.21891,
     "end_time": "2025-04-04T18:24:34.904396",
     "exception": false,
     "start_time": "2025-04-04T18:24:33.685486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: dataset_tool.py [OPTIONS]\r\n",
      "\r\n",
      "  Convert an image dataset into a dataset archive usable with StyleGAN2 ADA\r\n",
      "  PyTorch.\r\n",
      "\r\n",
      "  The input dataset format is guessed from the --source argument:\r\n",
      "\r\n",
      "  --source *_lmdb/                    Load LSUN dataset\r\n",
      "  --source cifar-10-python.tar.gz     Load CIFAR-10 dataset\r\n",
      "  --source train-images-idx3-ubyte.gz Load MNIST dataset\r\n",
      "  --source path/                      Recursively load all images from path/\r\n",
      "  --source dataset.zip                Recursively load all images from dataset.zip\r\n",
      "\r\n",
      "  Specifying the output format and path:\r\n",
      "\r\n",
      "  --dest /path/to/dir                 Save output files under /path/to/dir\r\n",
      "  --dest /path/to/dataset.zip         Save output files into /path/to/dataset.zip\r\n",
      "\r\n",
      "  The output dataset format can be either an image folder or an uncompressed\r\n",
      "  zip archive. Zip archives makes it easier to move datasets around file\r\n",
      "  servers and clusters, and may offer better training performance on network\r\n",
      "  file systems.\r\n",
      "\r\n",
      "  Images within the dataset archive will be stored as uncompressed PNG.\r\n",
      "  Uncompresed PNGs can be efficiently decoded in the training loop.\r\n",
      "\r\n",
      "  Class labels are stored in a file called 'dataset.json' that is stored at\r\n",
      "  the dataset root folder.  This file has the following structure:\r\n",
      "\r\n",
      "  {\r\n",
      "      \"labels\": [\r\n",
      "          [\"00000/img00000000.png\",6],\r\n",
      "          [\"00000/img00000001.png\",9],\r\n",
      "          ... repeated for every image in the datase\r\n",
      "          [\"00049/img00049999.png\",1]\r\n",
      "      ]\r\n",
      "  }\r\n",
      "\r\n",
      "  If the 'dataset.json' file cannot be found, the dataset is interpreted as\r\n",
      "  not containing class labels.\r\n",
      "\r\n",
      "  Image scale/crop and resolution requirements:\r\n",
      "\r\n",
      "  Output images must be square-shaped and they must all have the same power-\r\n",
      "  of-two dimensions.\r\n",
      "\r\n",
      "  To scale arbitrary input image size to a specific width and height, use\r\n",
      "  the --width and --height options.  Output resolution will be either the\r\n",
      "  original input resolution (if --width/--height was not specified) or the\r\n",
      "  one specified with --width/height.\r\n",
      "\r\n",
      "  Use the --transform=center-crop or --transform=center-crop-wide options to\r\n",
      "  apply a center crop transform on the input image.  These options should be\r\n",
      "  used with the --width and --height options.  For example:\r\n",
      "\r\n",
      "  python dataset_tool.py --source LSUN/raw/cat_lmdb --dest /tmp/lsun_cat \\\r\n",
      "      --transform=center-crop-wide --width 512 --height=384\r\n",
      "\r\n",
      "Options:\r\n",
      "  --source PATH                   Directory or archive name for input dataset\r\n",
      "                                  [required]\r\n",
      "\r\n",
      "  --dest PATH                     Output directory or archive name for output\r\n",
      "                                  dataset  [required]\r\n",
      "\r\n",
      "  --max-images INTEGER            Output only up to `max-images` images\r\n",
      "  --resize-filter [box|lanczos]   Filter to use when resizing images for\r\n",
      "                                  output resolution  [default: lanczos]\r\n",
      "\r\n",
      "  --transform [center-crop|center-crop-wide]\r\n",
      "                                  Input crop/resize mode\r\n",
      "  --width INTEGER                 Output width\r\n",
      "  --height INTEGER                Output height\r\n",
      "  --help                          Show this message and exit.\r\n"
     ]
    }
   ],
   "source": [
    "!python dataset_tool.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T18:24:34.961605Z",
     "iopub.status.busy": "2025-04-04T18:24:34.961061Z",
     "iopub.status.idle": "2025-04-04T18:26:30.777364Z",
     "shell.execute_reply": "2025-04-04T18:26:30.776837Z",
     "shell.execute_reply.started": "2025-04-04T14:23:10.880739Z"
    },
    "papermill": {
     "duration": 115.849062,
     "end_time": "2025-04-04T18:26:30.777504",
     "exception": false,
     "start_time": "2025-04-04T18:24:34.928442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python dataset_tool.py --source=\"/kaggle/input/batik-dataset-for-gan/Dataset Final\" --dest=./datasets/dataset.zip --width=128 --height=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T18:26:31.087500Z",
     "iopub.status.busy": "2025-04-04T18:26:31.086350Z",
     "iopub.status.idle": "2025-04-04T18:26:32.074250Z",
     "shell.execute_reply": "2025-04-04T18:26:32.073718Z",
     "shell.execute_reply.started": "2025-04-04T14:25:00.168891Z"
    },
    "papermill": {
     "duration": 1.145719,
     "end_time": "2025-04-04T18:26:32.074369",
     "exception": false,
     "start_time": "2025-04-04T18:26:30.928650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mDockerfile\u001b[0m*       \u001b[01;32mdataset_tool.py\u001b[0m*  \u001b[01;34mdocs\u001b[0m/         \u001b[01;32mprojector.py\u001b[0m*     \u001b[01;32mtrain.py\u001b[0m*\r\n",
      "\u001b[01;32mLICENSE.txt\u001b[0m*      \u001b[01;34mdatasets\u001b[0m/         \u001b[01;32mgenerate.py\u001b[0m*  \u001b[01;34mresults\u001b[0m/          \u001b[01;34mtraining\u001b[0m/\r\n",
      "\u001b[01;32mREADME.md\u001b[0m*        \u001b[01;34mdnnlib\u001b[0m/           \u001b[01;32mlegacy.py\u001b[0m*    \u001b[01;32mstyle_mixing.py\u001b[0m*\r\n",
      "\u001b[01;32mcalc_metrics.py\u001b[0m*  \u001b[01;32mdocker_run.sh\u001b[0m*    \u001b[01;34mmetrics\u001b[0m/      \u001b[01;34mtorch_utils\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T18:26:32.380000Z",
     "iopub.status.busy": "2025-04-04T18:26:32.375354Z",
     "iopub.status.idle": "2025-04-04T18:26:36.436120Z",
     "shell.execute_reply": "2025-04-04T18:26:36.436524Z",
     "shell.execute_reply.started": "2025-04-04T14:25:01.181886Z"
    },
    "papermill": {
     "duration": 4.211921,
     "end_time": "2025-04-04T18:26:36.436682",
     "exception": false,
     "start_time": "2025-04-04T18:26:32.224761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: train.py [OPTIONS]\r\n",
      "\r\n",
      "  Train a GAN using the techniques described in the paper \"Training\r\n",
      "  Generative Adversarial Networks with Limited Data\".\r\n",
      "\r\n",
      "  Examples:\r\n",
      "\r\n",
      "  # Train with custom dataset using 1 GPU.\r\n",
      "  python train.py --outdir=~/training-runs --data=~/mydataset.zip --gpus=1\r\n",
      "\r\n",
      "  # Train class-conditional CIFAR-10 using 2 GPUs.\r\n",
      "  python train.py --outdir=~/training-runs --data=~/datasets/cifar10.zip \\\r\n",
      "      --gpus=2 --cfg=cifar --cond=1\r\n",
      "\r\n",
      "  # Transfer learn MetFaces from FFHQ using 4 GPUs.\r\n",
      "  python train.py --outdir=~/training-runs --data=~/datasets/metfaces.zip \\\r\n",
      "      --gpus=4 --cfg=paper1024 --mirror=1 --resume=ffhq1024 --snap=10\r\n",
      "\r\n",
      "  # Reproduce original StyleGAN2 config F.\r\n",
      "  python train.py --outdir=~/training-runs --data=~/datasets/ffhq.zip \\\r\n",
      "      --gpus=8 --cfg=stylegan2 --mirror=1 --aug=noaug\r\n",
      "\r\n",
      "  Base configs (--cfg):\r\n",
      "    auto       Automatically select reasonable defaults based on resolution\r\n",
      "               and GPU count. Good starting point for new datasets.\r\n",
      "    stylegan2  Reproduce results for StyleGAN2 config F at 1024x1024.\r\n",
      "    paper256   Reproduce results for FFHQ and LSUN Cat at 256x256.\r\n",
      "    paper512   Reproduce results for BreCaHAD and AFHQ at 512x512.\r\n",
      "    paper1024  Reproduce results for MetFaces at 1024x1024.\r\n",
      "    cifar      Reproduce results for CIFAR-10 at 32x32.\r\n",
      "\r\n",
      "  Transfer learning source networks (--resume):\r\n",
      "    ffhq256        FFHQ trained at 256x256 resolution.\r\n",
      "    ffhq512        FFHQ trained at 512x512 resolution.\r\n",
      "    ffhq1024       FFHQ trained at 1024x1024 resolution.\r\n",
      "    celebahq256    CelebA-HQ trained at 256x256 resolution.\r\n",
      "    lsundog256     LSUN Dog trained at 256x256 resolution.\r\n",
      "    <PATH or URL>  Custom network pickle.\r\n",
      "\r\n",
      "Options:\r\n",
      "  --outdir DIR                    Where to save the results  [required]\r\n",
      "  --gpus INT                      Number of GPUs to use [default: 1]\r\n",
      "  --snap INT                      Snapshot interval [default: 50 ticks]\r\n",
      "  --metrics LIST                  Comma-separated list or \"none\" [default:\r\n",
      "                                  fid50k_full]\r\n",
      "\r\n",
      "  --seed INT                      Random seed [default: 0]\r\n",
      "  -n, --dry-run                   Print training options and exit\r\n",
      "  --data PATH                     Training data (directory or zip)  [required]\r\n",
      "  --cond BOOL                     Train conditional model based on dataset\r\n",
      "                                  labels [default: false]\r\n",
      "\r\n",
      "  --subset INT                    Train with only N images [default: all]\r\n",
      "  --mirror BOOL                   Enable dataset x-flips [default: false]\r\n",
      "  --cfg [auto|stylegan2|paper256|paper512|paper1024|cifar]\r\n",
      "                                  Base config [default: auto]\r\n",
      "  --gamma FLOAT                   Override R1 gamma\r\n",
      "  --kimg INT                      Override training duration\r\n",
      "  --batch INT                     Override batch size\r\n",
      "  --aug [noaug|ada|fixed]         Augmentation mode [default: ada]\r\n",
      "  --p FLOAT                       Augmentation probability for --aug=fixed\r\n",
      "  --target FLOAT                  ADA target value for --aug=ada\r\n",
      "  --augpipe [blit|geom|color|filter|noise|cutout|bg|bgc|bgcf|bgcfn|bgcfnc]\r\n",
      "                                  Augmentation pipeline [default: bgc]\r\n",
      "  --resume PKL                    Resume training [default: noresume]\r\n",
      "  --freezed INT                   Freeze-D [default: 0 layers]\r\n",
      "  --fp32 BOOL                     Disable mixed-precision training\r\n",
      "  --nhwc BOOL                     Use NHWC memory format with FP16\r\n",
      "  --nobench BOOL                  Disable cuDNN benchmarking\r\n",
      "  --allow-tf32 BOOL               Allow PyTorch to use TF32 internally\r\n",
      "  --workers INT                   Override number of DataLoader workers\r\n",
      "  --help                          Show this message and exit.\r\n"
     ]
    }
   ],
   "source": [
    "!python train.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T18:26:36.732583Z",
     "iopub.status.busy": "2025-04-04T18:26:36.732060Z",
     "iopub.status.idle": "2025-04-04T18:26:39.380986Z",
     "shell.execute_reply": "2025-04-04T18:26:39.381453Z",
     "shell.execute_reply.started": "2025-04-04T14:25:03.803565Z"
    },
    "papermill": {
     "duration": 2.798944,
     "end_time": "2025-04-04T18:26:39.381611",
     "exception": false,
     "start_time": "2025-04-04T18:26:36.582667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training options:\r\n",
      "{\r\n",
      "  \"num_gpus\": 1,\r\n",
      "  \"image_snapshot_ticks\": 50,\r\n",
      "  \"network_snapshot_ticks\": 50,\r\n",
      "  \"metrics\": [\r\n",
      "    \"fid50k_full\"\r\n",
      "  ],\r\n",
      "  \"random_seed\": 0,\r\n",
      "  \"training_set_kwargs\": {\r\n",
      "    \"class_name\": \"training.dataset.ImageFolderDataset\",\r\n",
      "    \"path\": \"./datasets/dataset.zip\",\r\n",
      "    \"use_labels\": false,\r\n",
      "    \"max_size\": 5330,\r\n",
      "    \"xflip\": false,\r\n",
      "    \"resolution\": 128\r\n",
      "  },\r\n",
      "  \"data_loader_kwargs\": {\r\n",
      "    \"pin_memory\": true,\r\n",
      "    \"num_workers\": 3,\r\n",
      "    \"prefetch_factor\": 2\r\n",
      "  },\r\n",
      "  \"G_kwargs\": {\r\n",
      "    \"class_name\": \"training.networks.Generator\",\r\n",
      "    \"z_dim\": 512,\r\n",
      "    \"w_dim\": 512,\r\n",
      "    \"mapping_kwargs\": {\r\n",
      "      \"num_layers\": 2\r\n",
      "    },\r\n",
      "    \"synthesis_kwargs\": {\r\n",
      "      \"channel_base\": 16384,\r\n",
      "      \"channel_max\": 512,\r\n",
      "      \"num_fp16_res\": 4,\r\n",
      "      \"conv_clamp\": 256\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"D_kwargs\": {\r\n",
      "    \"class_name\": \"training.networks.Discriminator\",\r\n",
      "    \"block_kwargs\": {},\r\n",
      "    \"mapping_kwargs\": {},\r\n",
      "    \"epilogue_kwargs\": {\r\n",
      "      \"mbstd_group_size\": 4\r\n",
      "    },\r\n",
      "    \"channel_base\": 16384,\r\n",
      "    \"channel_max\": 512,\r\n",
      "    \"num_fp16_res\": 4,\r\n",
      "    \"conv_clamp\": 256\r\n",
      "  },\r\n",
      "  \"G_opt_kwargs\": {\r\n",
      "    \"class_name\": \"torch.optim.Adam\",\r\n",
      "    \"lr\": 0.0025,\r\n",
      "    \"betas\": [\r\n",
      "      0,\r\n",
      "      0.99\r\n",
      "    ],\r\n",
      "    \"eps\": 1e-08\r\n",
      "  },\r\n",
      "  \"D_opt_kwargs\": {\r\n",
      "    \"class_name\": \"torch.optim.Adam\",\r\n",
      "    \"lr\": 0.0025,\r\n",
      "    \"betas\": [\r\n",
      "      0,\r\n",
      "      0.99\r\n",
      "    ],\r\n",
      "    \"eps\": 1e-08\r\n",
      "  },\r\n",
      "  \"loss_kwargs\": {\r\n",
      "    \"class_name\": \"training.loss.StyleGAN2Loss\",\r\n",
      "    \"r1_gamma\": 0.1024\r\n",
      "  },\r\n",
      "  \"total_kimg\": 25000,\r\n",
      "  \"batch_size\": 32,\r\n",
      "  \"batch_gpu\": 32,\r\n",
      "  \"ema_kimg\": 10.0,\r\n",
      "  \"ema_rampup\": 0.05,\r\n",
      "  \"ada_target\": 0.6,\r\n",
      "  \"augment_kwargs\": {\r\n",
      "    \"class_name\": \"training.augment.AugmentPipe\",\r\n",
      "    \"xflip\": 1,\r\n",
      "    \"rotate90\": 1,\r\n",
      "    \"xint\": 1,\r\n",
      "    \"scale\": 1,\r\n",
      "    \"rotate\": 1,\r\n",
      "    \"aniso\": 1,\r\n",
      "    \"xfrac\": 1,\r\n",
      "    \"brightness\": 1,\r\n",
      "    \"contrast\": 1,\r\n",
      "    \"lumaflip\": 1,\r\n",
      "    \"hue\": 1,\r\n",
      "    \"saturation\": 1\r\n",
      "  },\r\n",
      "  \"run_dir\": \"./results/00000-dataset-auto1\"\r\n",
      "}\r\n",
      "\r\n",
      "Output directory:   ./results/00000-dataset-auto1\r\n",
      "Training data:      ./datasets/dataset.zip\r\n",
      "Training duration:  25000 kimg\r\n",
      "Number of GPUs:     1\r\n",
      "Number of images:   5330\r\n",
      "Image resolution:   128\r\n",
      "Conditional model:  False\r\n",
      "Dataset x-flips:    False\r\n",
      "\r\n",
      "Dry run; exiting.\r\n"
     ]
    }
   ],
   "source": [
    "!python train.py --outdir=./results --data=./datasets/dataset.zip --dry-run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-04T14:38:10.378Z",
     "iopub.execute_input": "2025-04-04T18:26:39.681212Z",
     "iopub.status.busy": "2025-04-04T18:26:39.676938Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-04-04T18:26:39.527291",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python train.py --outdir=./results --data=./datasets/dataset.zip --snap=5 --kimg=1000"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6987767,
     "sourceId": 11193215,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30068,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-04T18:23:53.599147",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
