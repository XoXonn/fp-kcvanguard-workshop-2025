{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c87ea320",
   "metadata": {
    "papermill": {
     "duration": 0.006988,
     "end_time": "2025-04-13T03:17:39.858272",
     "exception": false,
     "start_time": "2025-04-13T03:17:39.851284",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# road to semuanya di acc kcv\n",
    "\n",
    "todo:\n",
    "- [x] dataset implementation\n",
    "- [x] model\n",
    "- [x] training\n",
    "- [x] deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4ae98c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T03:17:39.871865Z",
     "iopub.status.busy": "2025-04-13T03:17:39.871553Z",
     "iopub.status.idle": "2025-04-13T03:17:44.358425Z",
     "shell.execute_reply": "2025-04-13T03:17:44.357459Z"
    },
    "papermill": {
     "duration": 4.495364,
     "end_time": "2025-04-13T03:17:44.359962",
     "exception": false,
     "start_time": "2025-04-13T03:17:39.864598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install -q onnx torchinfo torchmetrics[image]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272c6452",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-13T03:17:44.373807Z",
     "iopub.status.busy": "2025-04-13T03:17:44.373559Z",
     "iopub.status.idle": "2025-04-13T03:17:55.362857Z",
     "shell.execute_reply": "2025-04-13T03:17:55.362168Z"
    },
    "papermill": {
     "duration": 10.997873,
     "end_time": "2025-04-13T03:17:55.364554",
     "exception": false,
     "start_time": "2025-04-13T03:17:44.366681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchinfo import summary\n",
    "from torchmetrics.image.inception import InceptionScore\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.image.perceptual_path_length import PerceptualPathLength\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Any, Callable, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abbadf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T03:17:55.378554Z",
     "iopub.status.busy": "2025-04-13T03:17:55.378172Z",
     "iopub.status.idle": "2025-04-13T03:17:55.433814Z",
     "shell.execute_reply": "2025-04-13T03:17:55.432941Z"
    },
    "papermill": {
     "duration": 0.063838,
     "end_time": "2025-04-13T03:17:55.435160",
     "exception": false,
     "start_time": "2025-04-13T03:17:55.371322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# General\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "RANDOM_SEED = 69\n",
    "\n",
    "# Dataloader\n",
    "DATASET_PATH = '/kaggle/input/batik-dataset-for-gan/Dataset Final'\n",
    "BATCH_SIZES = [16, 16, 8, 8, 4, 4, 4]\n",
    "NUM_WORKERS = 4\n",
    "SHUFFLE = True\n",
    "PIN_MEMORY = False\n",
    "\n",
    "# Modelling\n",
    "LATENT_FEATURES = 512\n",
    "RESOLUTION = 128\n",
    "\n",
    "# Training\n",
    "LEARNING_RATE = 2e-3\n",
    "NUM_EPOCHS = 120\n",
    "R1_GAMMA = 10\n",
    "OUTPUT_DIR = 'generated_images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a171f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T03:17:55.448419Z",
     "iopub.status.busy": "2025-04-13T03:17:55.448192Z",
     "iopub.status.idle": "2025-04-13T03:17:55.456610Z",
     "shell.execute_reply": "2025-04-13T03:17:55.456045Z"
    },
    "papermill": {
     "duration": 0.016372,
     "end_time": "2025-04-13T03:17:55.457823",
     "exception": false,
     "start_time": "2025-04-13T03:17:55.441451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.deterministic = True\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "torch.autograd.profiler.profile(False)\n",
    "torch.autograd.profiler.emit_nvtx(False)\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8cb734",
   "metadata": {
    "papermill": {
     "duration": 0.005881,
     "end_time": "2025-04-13T03:17:55.469932",
     "exception": false,
     "start_time": "2025-04-13T03:17:55.464051",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715992e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T03:17:55.482939Z",
     "iopub.status.busy": "2025-04-13T03:17:55.482682Z",
     "iopub.status.idle": "2025-04-13T03:17:55.488662Z",
     "shell.execute_reply": "2025-04-13T03:17:55.487845Z"
    },
    "papermill": {
     "duration": 0.013866,
     "end_time": "2025-04-13T03:17:55.489957",
     "exception": false,
     "start_time": "2025-04-13T03:17:55.476091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BatikGANDataset(Dataset):\n",
    "    '''\n",
    "    BatikGAN Dataset Implementation with lazy loading.\n",
    "    \n",
    "    Args:\n",
    "        path (str): Path to image directory.\n",
    "        transform (callable, optional): Image transforms that takes a PIL.Image as input. Default value is None.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, path: str, transform: Optional[Callable[Image.Image, Any]] = None):\n",
    "        super(BatikGANDataset, self).__init__()\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        self.files = [ f for f in os.listdir(self.path) if f.endswith(('.png', '.jpg', '.jpeg')) ]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, index: int) -> torch.Tensor:\n",
    "        img_path = os.path.join(self.path, self.files[index])\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e5897a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T03:17:55.503026Z",
     "iopub.status.busy": "2025-04-13T03:17:55.502759Z",
     "iopub.status.idle": "2025-04-13T03:17:55.506801Z",
     "shell.execute_reply": "2025-04-13T03:17:55.506042Z"
    },
    "papermill": {
     "duration": 0.011857,
     "end_time": "2025-04-13T03:17:55.508058",
     "exception": false,
     "start_time": "2025-04-13T03:17:55.496201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_loader(resolution) -> DataLoader:\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.Resize(resolution),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5] * 3, [0.5] * 3),\n",
    "    ])\n",
    "    dataset = BatikGANDataset(DATASET_PATH, transform=transform)\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZES[int(math.log2(resolution)) - 2], shuffle=SHUFFLE, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1f70db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T03:17:55.521097Z",
     "iopub.status.busy": "2025-04-13T03:17:55.520852Z",
     "iopub.status.idle": "2025-04-13T03:17:56.305555Z",
     "shell.execute_reply": "2025-04-13T03:17:56.304553Z"
    },
    "papermill": {
     "duration": 0.801848,
     "end_time": "2025-04-13T03:17:56.316193",
     "exception": false,
     "start_time": "2025-04-13T03:17:55.514345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch = next(iter(get_loader(RESOLUTION)))\n",
    "grid = make_grid(batch, nrow=math.ceil(BATCH_SIZES[-1] ** .5), normalize=True)\n",
    "grid_np = grid.numpy().transpose((1, 2, 0))\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(grid_np)\n",
    "plt.axis('off')\n",
    "plt.title('Batch of Images from Batik GAN Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef77264",
   "metadata": {
    "papermill": {
     "duration": 0.016568,
     "end_time": "2025-04-13T03:17:56.349878",
     "exception": false,
     "start_time": "2025-04-13T03:17:56.333310",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1c255f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T03:17:56.384355Z",
     "iopub.status.busy": "2025-04-13T03:17:56.384083Z",
     "iopub.status.idle": "2025-04-13T03:17:56.389765Z",
     "shell.execute_reply": "2025-04-13T03:17:56.388930Z"
    },
    "papermill": {
     "duration": 0.024653,
     "end_time": "2025-04-13T03:17:56.391075",
     "exception": false,
     "start_time": "2025-04-13T03:17:56.366422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WSLinear(nn.Module):\n",
    "    '''\n",
    "    Weighted scale linear for equalized learning rate.\n",
    "\n",
    "    Args:\n",
    "        in_features (int): The number of input features.\n",
    "        out_features (int): The number of output features.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int) -> None:\n",
    "        super(WSLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        self.linear = nn.Linear(self.in_features, self.out_features)\n",
    "        self.scale = (2 / self.in_features) ** 0.5\n",
    "        self.bias = self.linear.bias\n",
    "        self.linear.bias = None\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self) -> None:\n",
    "        nn.init.normal_(self.linear.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear(x * self.scale) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34019bd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T03:17:56.425328Z",
     "iopub.status.busy": "2025-04-13T03:17:56.425101Z",
     "iopub.status.idle": "2025-04-13T03:17:56.429653Z",
     "shell.execute_reply": "2025-04-13T03:17:56.429069Z"
    },
    "papermill": {
     "duration": 0.022996,
     "end_time": "2025-04-13T03:17:56.430828",
     "exception": false,
     "start_time": "2025-04-13T03:17:56.407832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WSConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Weight-scaled Conv2d layer for equalized learning rate.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input channels.\n",
    "        out_channels (int): Number of output channels.\n",
    "        kernel_size (int, optional): Size of the convolving kernel. Default: 3.\n",
    "        stride (int, optional): Stride of the convolution. Default: 1.\n",
    "        padding (int, optional): Padding added to all sides of the input. Default: 1.\n",
    "        gain (float, optional): Gain factor for weight initialization. Default: 2.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, gain=2):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.scale = (gain / (in_channels * kernel_size ** 2)) ** 0.5\n",
    "        self.bias = self.conv.bias\n",
    "        self.conv.bias = None  # Remove bias to apply it after scaling\n",
    "\n",
    "        # Initialize weights\n",
    "        nn.init.normal_(self.conv.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846ca049",
   "metadata": {
    "papermill": {
     "duration": 0.017104,
     "end_time": "2025-04-13T03:17:56.464685",
     "exception": false,
     "start_time": "2025-04-13T03:17:56.447581",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Generator StyleGAN\n",
    "\n",
    "Referensi:\n",
    "- [StyleGAN](https://arxiv.org/pdf/1812.04948)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f66c3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T03:17:56.500053Z",
     "iopub.status.busy": "2025-04-13T03:17:56.499781Z",
     "iopub.status.idle": "2025-04-13T03:17:56.505007Z",
     "shell.execute_reply": "2025-04-13T03:17:56.504200Z"
    },
    "papermill": {
     "duration": 0.024356,
     "end_time": "2025-04-13T03:17:56.506295",
     "exception": false,
     "start_time": "2025-04-13T03:17:56.481939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Mapping(nn.Module):\n",
    "    '''\n",
    "    Mapping network.\n",
    "\n",
    "    Args:\n",
    "        features (int): Number of features in the input and output.\n",
    "        num_layers (int): Number of layers in the feed forward network.\n",
    "        num_styles (int): Number of styles to generate.\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        features: int,\n",
    "        num_styles: int,\n",
    "        num_layers: int = 8,\n",
    "    ) -> None:\n",
    "        super(Mapping, self).__init__()\n",
    "        self.features = features\n",
    "        self.num_layers = num_layers\n",
    "        self.num_styles = num_styles\n",
    "\n",
    "        layers = []\n",
    "        for _ in range(self.num_layers):\n",
    "            layers.append(WSLinear(self.features, self.features))\n",
    "            layers.append(nn.LeakyReLU(0.2))\n",
    "\n",
    "        self.fc = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (b, l).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (b, n, l).\n",
    "        '''\n",
    "\n",
    "        x = self.fc(x) # (b, l)\n",
    "        x = x.unsqueeze(1).expand(-1, self.num_styles, -1) # (b, n, l)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5f0bbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T03:17:56.540181Z",
     "iopub.status.busy": "2025-04-13T03:17:56.539925Z",
     "iopub.status.idle": "2025-04-13T03:17:56.544700Z",
     "shell.execute_reply": "2025-04-13T03:17:56.544129Z"
    },
    "papermill": {
     "duration": 0.023006,
     "end_time": "2025-04-13T03:17:56.545773",
     "exception": false,
     "start_time": "2025-04-13T03:17:56.522767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AdaIN(nn.Module):\n",
    "    '''\n",
    "    Adaptive Instance Normalization (AdaIN)\n",
    "    AdaIN(x_i, y) = y_s,i * (x_i - mean(x_i)) / std(x_i) + y_b,i\n",
    "\n",
    "    Args:\n",
    "        eps (float, optional): Small value to avoid division by zero. Default value is 0.00001.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, eps: float= 1e-5) -> None:\n",
    "        super(AdaIN, self).__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        scale: torch.Tensor,\n",
    "        shift: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        '''\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (b, c, h, w).\n",
    "            scale (torch.Tensor): Scale tensor of shape (b, c).\n",
    "            shift (torch.Tensor): Shift tensor of shape (b, c).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (b, c, h, w).\n",
    "        '''\n",
    "\n",
    "        b, c, *_ = x.shape\n",
    "\n",
    "        mean = x.mean(dim=(2, 3), keepdim=True) # (b, c, 1, 1)\n",
    "        var = x.var(dim=(2, 3), keepdim=True) # (b, c, 1, 1)\n",
    "        x_norm = (x - mean) / (var + self.eps) ** .5 # (b, c, h, w)\n",
    "\n",
    "        scale = scale.view(b, c, 1, 1) # (b, c, 1, 1)\n",
    "        shift = shift.view(b, c, 1, 1) # (b, c, 1, 1)\n",
    "        outputs = scale * x_norm + shift # (b, c, h, w)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb19b71a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T03:17:56.580169Z",
     "iopub.status.busy": "2025-04-13T03:17:56.579905Z",
     "iopub.status.idle": "2025-04-13T03:17:56.586770Z",
     "shell.execute_reply": "2025-04-13T03:17:56.586137Z"
    },
    "papermill": {
     "duration": 0.025543,
     "end_time": "2025-04-13T03:17:56.588014",
     "exception": false,
     "start_time": "2025-04-13T03:17:56.562471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SynthesisLayer(nn.Module):\n",
    "    '''\n",
    "    Synthesis network layer which consist of:\n",
    "    - Conv2d.\n",
    "    - AdaIN.\n",
    "    - Affine transformation.\n",
    "    - Noise injection.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): The number of input channels.\n",
    "        out_channels (int): The number of output channels.\n",
    "        latent_features (int): The number of latent features.\n",
    "        use_conv (bool, optional): Whether to use convolution or not. Default value is True.\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        latent_features: int,\n",
    "        use_conv: bool = True\n",
    "    ) -> None:\n",
    "        super(SynthesisLayer, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.latent_features = latent_features\n",
    "        self.use_conv = use_conv\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            WSConv2d(self.in_channels, self.out_channels, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        ) if self.use_conv else nn.Identity()\n",
    "        self.norm = AdaIN()\n",
    "        self.scale_transform = WSLinear(self.latent_features, self.out_channels)\n",
    "        self.shift_transform = WSLinear(self.latent_features, self.out_channels)\n",
    "        self.noise_factor = nn.Parameter(torch.zeros(1, self.out_channels, 1, 1))\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self) -> None:\n",
    "        nn.init.ones_(self.scale_transform.bias)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        w: torch.Tensor,\n",
    "        noise: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        '''\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (b, c, h, w).\n",
    "            w (torch.Tensor): Latent space vector of shape (b, l).\n",
    "            noise (torch.Tensor, optional): Noise tensor of shape (b, 1, h, w). Default value is None.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (b, c, h, w).\n",
    "        '''\n",
    "\n",
    "        b, _, h, w_ = x.shape\n",
    "        x = self.conv(x) # (b, o_c, h, w)\n",
    "        if noise is None:\n",
    "            noise = torch.randn(b, 1, h, w_, device=x.device) # (b, 1, h, w)\n",
    "        x += self.noise_factor * noise # (b, o_c, h, w)\n",
    "        y_s = self.scale_transform(w) # (b, o_c)\n",
    "        y_b = self.shift_transform(w) # (b, o_c)\n",
    "        x = self.norm(x, y_s, y_b) # (b, i_c, h, w)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928e1d02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T03:17:56.623639Z",
     "iopub.status.busy": "2025-04-13T03:17:56.623382Z",
     "iopub.status.idle": "2025-04-13T03:17:56.628885Z",
     "shell.execute_reply": "2025-04-13T03:17:56.628248Z"
    },
    "papermill": {
     "duration": 0.024919,
     "end_time": "2025-04-13T03:17:56.630185",
     "exception": false,
     "start_time": "2025-04-13T03:17:56.605266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SynthesisBlock(nn.Module):\n",
    "    '''\n",
    "    Synthesis network block which consist of:\n",
    "    - Optional upsampling.\n",
    "    - 2 Synthesis Layers.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): The number of input channels.\n",
    "        out_channels (int): The number of output channels.\n",
    "        latent_features (int): The number of latent features.\n",
    "        use_conv (bool, optional): Whether to use convolution or not. Default value is True.\n",
    "        upsample (bool, optional): Whether to use upsampling or not. Default value is True.\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        latent_features: int,\n",
    "        *,\n",
    "        use_conv: bool = True,\n",
    "        upsample: bool = True\n",
    "     ) -> None:\n",
    "        super(SynthesisBlock, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.latent_features = latent_features\n",
    "        self.use_conv = use_conv\n",
    "        self.upsample = upsample\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear') if self.upsample else nn.Identity()\n",
    "        self.layers = nn.ModuleList([\n",
    "            SynthesisLayer(self.in_channels, self.in_channels, self.latent_features, use_conv=self.use_conv),\n",
    "            SynthesisLayer(self.in_channels, self.out_channels, self.latent_features)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x: torch.Tensor, w: torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (b, c, h, w).\n",
    "            w (torch.Tensor): Latent vector of shape (b, 2, l).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (b, c, h, w) if not upsample else (b, c, 2h, 2w).\n",
    "        '''\n",
    "\n",
    "        x = self.upsample(x) # (b, c, h, w) if not upsample else (b, c, 2h, 2w)\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x, w[:, i]) # (b, c, h, w) if not upsample else (b, c, 2h, 2w)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ad0574",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T03:17:56.664440Z",
     "iopub.status.busy": "2025-04-13T03:17:56.664219Z",
     "iopub.status.idle": "2025-04-13T03:17:56.673060Z",
     "shell.execute_reply": "2025-04-13T03:17:56.672409Z"
    },
    "papermill": {
     "duration": 0.027345,
     "end_time": "2025-04-13T03:17:56.674254",
     "exception": false,
     "start_time": "2025-04-13T03:17:56.646909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Synthesis(nn.Module):\n",
    "    '''\n",
    "    Synthesis network which consist of:\n",
    "    - Constant tensor.\n",
    "    - Synthesis blocks.\n",
    "    - ToRGB convolutions.\n",
    "\n",
    "    Args:\n",
    "        resolution (int): The resolution of the image.\n",
    "        const_channels (int): The number of channels in the constant tensor. Default value is 512.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, resolution: int, const_channels: int = 512) -> None:\n",
    "        super(Synthesis, self).__init__()\n",
    "        self.const_channels = const_channels\n",
    "        self.resolution = resolution\n",
    "\n",
    "        self.resolution_levels = int(math.log2(resolution)) - 1\n",
    "\n",
    "        self.constant = nn.Parameter(torch.ones(1, self.const_channels, 4, 4)) # (c, 4, 4)\n",
    "\n",
    "        in_channels = self.const_channels\n",
    "        blocks = [ SynthesisBlock(in_channels, in_channels, self.const_channels, use_conv=False, upsample=False) ]\n",
    "        to_rgb = [ WSConv2d(in_channels, 3, kernel_size=1, padding=0) ]\n",
    "\n",
    "        for _ in range(3):\n",
    "            blocks.append(SynthesisBlock(in_channels, in_channels, self.const_channels))\n",
    "            to_rgb.append(WSConv2d(in_channels, 3, kernel_size=1, padding=0))\n",
    "\n",
    "        for _ in range(self.resolution_levels - 4):\n",
    "            blocks.append(SynthesisBlock(in_channels, in_channels // 2, self.const_channels))\n",
    "            to_rgb.append(WSConv2d(in_channels // 2, 3, kernel_size=1, padding=0))\n",
    "            in_channels //= 2\n",
    "\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "        self.to_rgb = nn.ModuleList(to_rgb)\n",
    "\n",
    "    def forward(self, w: torch.Tensor, alpha: float = 1, steps: int = None) -> torch.Tensor:\n",
    "        '''\n",
    "        Args:\n",
    "            w (torch.Tensor): Latent space vector of shape (b, n, l).\n",
    "            alpha (float): Fade in alpha value. Default value is 1.\n",
    "            steps (int): The number of steps starting from 0. If None, then the maximum number of steps is used. Default value is None.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (b, 3, h, w).\n",
    "        '''\n",
    "\n",
    "        b = w.size(0)\n",
    "        x = self.constant.expand(b, -1, -1, -1).clone() # (b, c, h, w)\n",
    "\n",
    "        if steps is None:\n",
    "            steps = int(math.log2(RESOLUTION) - 2)\n",
    "\n",
    "        if steps == 0:\n",
    "            x = self.blocks[0](x, w[:, :2]) # (b, c, h, w)\n",
    "            x = self.to_rgb[0](x) # (b, c, h, w)\n",
    "            return x\n",
    "        \n",
    "        for i in range(steps):\n",
    "            x = self.blocks[i](x, w[:, 2 * i : 2 * (i + 1)]) # (b, c, h/2, w/2)\n",
    "\n",
    "        if alpha < 1:\n",
    "            old_rgb = self.to_rgb[steps - 1](x) # (b, 3, h/2, w/2)\n",
    "            old_rgb = F.interpolate(old_rgb, scale_factor=2, mode='bilinear', align_corners=False) # (b, 3, h, w)\n",
    "\n",
    "        x = self.blocks[steps](x, w[:, 2 * steps : 2 * (steps + 1), :]) # (b, 3, h, w)\n",
    "        new_rgb = self.to_rgb[steps](x) # (b, 3, h, w)\n",
    "\n",
    "        if alpha < 1:\n",
    "            x = (1 - alpha) * old_rgb + alpha * new_rgb # (b, 3, h, w)\n",
    "        else:\n",
    "            x = new_rgb # (b, 3, h, w)\n",
    "        \n",
    "        x = x.tanh() # (b, 3, h, w)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68456948",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T03:17:56.709045Z",
     "iopub.status.busy": "2025-04-13T03:17:56.708793Z",
     "iopub.status.idle": "2025-04-13T03:17:56.713886Z",
     "shell.execute_reply": "2025-04-13T03:17:56.713295Z"
    },
    "papermill": {
     "duration": 0.024255,
     "end_time": "2025-04-13T03:17:56.715201",
     "exception": false,
     "start_time": "2025-04-13T03:17:56.690946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Truncation(nn.Module):\n",
    "    '''\n",
    "    Truncation trick method.\n",
    "    w' = w_com + psi * (w_com - w)\n",
    "    \n",
    "    Args:\n",
    "        w_dim (int): The number of latent features.\n",
    "        psi (float): Moving average linear interpolation coefficient.\n",
    "        beta (float, optional): Exponential moving average update coefficient. Default value is 0.9.\n",
    "        max_layers (int, optional): The maximum number of layers affected by truncation trick. Default value is 8.\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        w_dim: int,\n",
    "        psi: float, \n",
    "        beta: float = 0.9,\n",
    "        max_layers: int = 8\n",
    "    ) -> None:\n",
    "        assert psi < 1\n",
    "        super(Truncation, self).__init__()\n",
    "        self.w_dim = w_dim\n",
    "        self.psi = psi\n",
    "        self.beta = beta\n",
    "        self.max_layers = max_layers\n",
    "        \n",
    "        self.register_buffer('w_avg', torch.zeros(self.w_dim))\n",
    "\n",
    "    def update(self, new_w_avg: torch.Tensor) -> None:\n",
    "        self.w_avg = self.beta * self.w_avg + (1 - self.beta) * new_w_avg\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        lerp = torch.lerp(self.w_avg, x, self.psi) # (b, n, l)\n",
    "        trunc = (torch.arange(x.size(1)) < self.max_layers).view(1, -1, 1).to(x.device) # (1, n, 1)\n",
    "        x = torch.where(trunc, lerp, x) # (b, n, l)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105ffbb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T03:17:56.749116Z",
     "iopub.status.busy": "2025-04-13T03:17:56.748861Z",
     "iopub.status.idle": "2025-04-13T03:17:56.755646Z",
     "shell.execute_reply": "2025-04-13T03:17:56.755058Z"
    },
    "papermill": {
     "duration": 0.024915,
     "end_time": "2025-04-13T03:17:56.756714",
     "exception": false,
     "start_time": "2025-04-13T03:17:56.731799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StyleGAN(nn.Module):\n",
    "    '''\n",
    "    StyleGAN implementation.\n",
    "\n",
    "    Args:\n",
    "        num_features (int): The number of features in the latent space vector.\n",
    "        resolution (int): The resolution of the image.\n",
    "        mix_reg_p (float, optional): Style mixing regularization probability. Default value is 0.9.\n",
    "        trunc_thresh (float, optional): Truncation trick threshold. Default value is 0.7.\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_features: int, \n",
    "        resolution: int, \n",
    "        mix_reg_p: float = 0.9,\n",
    "        trunc_thresh: float = 0.7\n",
    "    ) -> None:\n",
    "        super(StyleGAN, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.resolution = resolution\n",
    "        self.num_blocks = int(math.log2(self.resolution)) - 1\n",
    "        self.num_layers = 2 * self.num_blocks\n",
    "        self.mix_reg_p = mix_reg_p\n",
    "        self.trunc_thresh = trunc_thresh\n",
    "\n",
    "        self.mapping = Mapping(self.num_features, self.num_layers)\n",
    "        self.synthesis = Synthesis(self.resolution, self.num_features)\n",
    "        self.truncation = Truncation(self.num_features, self.trunc_thresh)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor, \n",
    "        alpha: float = 1, \n",
    "        steps: int = None\n",
    "    ) -> torch.Tensor:\n",
    "        '''\n",
    "        Args:\n",
    "            x (torch.Tensor): Random input tensor of shape (b, l).\n",
    "            alpha (float, optional): Fade in alpha value. Default value is 1.\n",
    "            steps (int, optional): The number of steps starting from 0. If None, then the maximum number of steps is used. Default value is None.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (b, c, h, w).\n",
    "        '''\n",
    "\n",
    "        if steps is None:\n",
    "            steps = int(math.log2(self.resolution) - 2)\n",
    "\n",
    "        w = self.mapping(x) # (b, n, l)\n",
    "\n",
    "        if self.training:\n",
    "            if self.mix_reg_p > 0 and torch.rand(1).item() < self.mix_reg_p:\n",
    "                x2 = torch.randn_like(x, device=x.device) # (b, l)\n",
    "                w2 = self.mapping(x2) # (b, n, l)\n",
    "                mixing_cutoff = torch.randint(1, self.num_layers + 1, (1,)).item()\n",
    "                layer_indices = torch.arange(self.num_layers, device=w.device).view(1, self.num_layers, 1) # (1, n, 1)\n",
    "                w = torch.where(layer_indices < mixing_cutoff, w, w2) # (b, n, l)\n",
    "        else:\n",
    "            self.truncation.update(w.mean(dim=(0, 1)))\n",
    "            w = self.truncation(w) # (b, n, l)\n",
    "        \n",
    "        outputs = self.synthesis(w, alpha, steps) # (b, c, h, w)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178d2b1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T03:17:56.790809Z",
     "iopub.status.busy": "2025-04-13T03:17:56.790597Z",
     "iopub.status.idle": "2025-04-13T03:17:59.016140Z",
     "shell.execute_reply": "2025-04-13T03:17:59.015243Z"
    },
    "papermill": {
     "duration": 2.24413,
     "end_time": "2025-04-13T03:17:59.017707",
     "exception": false,
     "start_time": "2025-04-13T03:17:56.773577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator = StyleGAN(LATENT_FEATURES, RESOLUTION).to(DEVICE)\n",
    "\n",
    "summary(generator, input_data=[\n",
    "    torch.randn(BATCH_SIZES[-1], LATENT_FEATURES),\n",
    "    0.5,\n",
    "    int(math.log2(RESOLUTION) - 2)\n",
    "], mode='train', device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71304a45",
   "metadata": {
    "papermill": {
     "duration": 0.019188,
     "end_time": "2025-04-13T03:17:59.060970",
     "exception": false,
     "start_time": "2025-04-13T03:17:59.041782",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Discriminator ProGAN\n",
    "Intinya terdiri dari:\n",
    "- Progressive growing (Progressive Downsampling)\n",
    "- PixelNorm\n",
    "- Equalized LR\n",
    "- MiniBatch Std\n",
    "\n",
    "Referensi: https://www.youtube.com/watch?v=nkQHASviYac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca34bbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T03:17:59.099825Z",
     "iopub.status.busy": "2025-04-13T03:17:59.099540Z",
     "iopub.status.idle": "2025-04-13T03:17:59.103808Z",
     "shell.execute_reply": "2025-04-13T03:17:59.103149Z"
    },
    "papermill": {
     "duration": 0.025349,
     "end_time": "2025-04-13T03:17:59.105103",
     "exception": false,
     "start_time": "2025-04-13T03:17:59.079754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional block with two WSConv2d layers.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input channels.\n",
    "        out_channels (int): Number of output channels.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = WSConv2d(in_channels, out_channels)\n",
    "        self.conv2 = WSConv2d(out_channels, out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky(self.conv1(x))\n",
    "        x = self.leaky(self.conv2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aba5955",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T03:17:59.145527Z",
     "iopub.status.busy": "2025-04-13T03:17:59.145275Z",
     "iopub.status.idle": "2025-04-13T03:17:59.155288Z",
     "shell.execute_reply": "2025-04-13T03:17:59.154450Z"
    },
    "papermill": {
     "duration": 0.030503,
     "end_time": "2025-04-13T03:17:59.156599",
     "exception": false,
     "start_time": "2025-04-13T03:17:59.126096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ProGANDiscriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Progressive GAN Discriminator compatible with StyleGAN generator.\n",
    "\n",
    "    Args:\n",
    "        resolution (int): Target resolution of the images.\n",
    "        features (int): Base number of features/channels.\n",
    "        img_channels (int, optional): Number of image channels. Default: 3.\n",
    "    \"\"\"\n",
    "    def __init__(self, resolution, features=512, img_channels=3):\n",
    "        super(ProGANDiscriminator, self).__init__()\n",
    "        self.resolution = resolution\n",
    "        self.features = features\n",
    "        self.img_channels = img_channels\n",
    "        self.resolution_levels = int(math.log2(resolution) - 1)\n",
    "\n",
    "        # Create progressive blocks and RGB layers\n",
    "        self.prog_blocks, self.rgb_layers = nn.ModuleList([]), nn.ModuleList([])\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "\n",
    "        # Calculate channel dimensions for each resolution level\n",
    "        # Start with minimum channels at highest resolution, double when going down\n",
    "        current_channels = features // (2 ** (self.resolution_levels - 4))\n",
    "        for i in range(self.resolution_levels, 0, -1):\n",
    "            # For first 4 resolution levels, keep channel count the same as base features\n",
    "            next_channels = features if i <= 4 else current_channels * 2\n",
    "\n",
    "            # Add conv block for this resolution level\n",
    "            self.prog_blocks.append(ConvBlock(current_channels, next_channels))\n",
    "            # Add RGB conversion layer for this resolution level\n",
    "            self.rgb_layers.append(\n",
    "                WSConv2d(img_channels, current_channels, kernel_size=1, stride=1, padding=0)\n",
    "            )\n",
    "\n",
    "            current_channels = next_channels\n",
    "\n",
    "        # For 4x4 resolution\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)  # Downsampling\n",
    "\n",
    "        # Final block for 4x4 resolution with minibatch std\n",
    "        self.final_block = nn.Sequential(\n",
    "            WSConv2d(features + 1, features, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(features, features, kernel_size=4, padding=0, stride=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(features, 1, kernel_size=1, padding=0, stride=1),\n",
    "        )\n",
    "\n",
    "    def minibatch_std(self, x):\n",
    "        \"\"\"Add minibatch standard deviation channel to the feature maps.\"\"\"\n",
    "        batch_statistics = torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])\n",
    "        return torch.cat([x, batch_statistics], dim=1)\n",
    "\n",
    "    def forward(self, x, alpha=1.0, current_step=0):\n",
    "        \"\"\"\n",
    "        Forward pass through the discriminator.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input images of shape (b, 3, h, w).\n",
    "            alpha (float): Fade-in factor for progressive growing [0, 1].\n",
    "            current_step (int): Current step in progressive growing (0 = lowest resolution).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Discriminator output of shape (b, 1).\n",
    "        \"\"\"\n",
    "        # Determine which layers to use based on the current step\n",
    "        step_index = self.resolution_levels - current_step - 1\n",
    "\n",
    "        # Process the input through the appropriate RGB layer\n",
    "        out = self.leaky(self.rgb_layers[step_index](x))\n",
    "\n",
    "        # If we're at 4x4 resolution (final step)\n",
    "        if step_index == self.resolution_levels - 1:\n",
    "            out = self.minibatch_std(out)\n",
    "            return self.final_block(out).view(out.shape[0], -1)\n",
    "\n",
    "        # Process through appropriate conv block\n",
    "        out = self.prog_blocks[step_index](out)\n",
    "        out = self.avg_pool(out)  # Downsample\n",
    "\n",
    "        # Handle fade-in if alpha < 1\n",
    "        if alpha < 1:\n",
    "            # Process the downsampled input through the next RGB layer\n",
    "            downscaled = self.avg_pool(x)\n",
    "            y = self.leaky(self.rgb_layers[step_index + 1](downscaled))\n",
    "            out = alpha * out + (1 - alpha) * y\n",
    "\n",
    "        # Continue processing through the remaining blocks\n",
    "        for i in range(step_index + 1, self.resolution_levels):\n",
    "            out = self.prog_blocks[i](out)\n",
    "            if i < self.resolution_levels - 1:  # Don't downsample at the final resolution\n",
    "                out = self.avg_pool(out)\n",
    "\n",
    "        # Final processing at 4x4 resolution\n",
    "        out = self.minibatch_std(out)\n",
    "        return self.final_block(out).view(out.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d152fc13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T03:17:59.194395Z",
     "iopub.status.busy": "2025-04-13T03:17:59.194176Z",
     "iopub.status.idle": "2025-04-13T03:18:00.474742Z",
     "shell.execute_reply": "2025-04-13T03:18:00.473821Z"
    },
    "papermill": {
     "duration": 1.300734,
     "end_time": "2025-04-13T03:18:00.476034",
     "exception": false,
     "start_time": "2025-04-13T03:17:59.175300",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "discriminator = ProGANDiscriminator(RESOLUTION, 512).to(DEVICE)\n",
    "\n",
    "summary(\n",
    "    discriminator,\n",
    "    input_data=[\n",
    "        torch.randn(2, 3, RESOLUTION, RESOLUTION),\n",
    "        1.0,\n",
    "        int(math.log2(RESOLUTION)) - 2\n",
    "    ],\n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841beb9d",
   "metadata": {
    "papermill": {
     "duration": 0.019497,
     "end_time": "2025-04-13T03:18:00.515265",
     "exception": false,
     "start_time": "2025-04-13T03:18:00.495768",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebdb522",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T03:18:00.554255Z",
     "iopub.status.busy": "2025-04-13T03:18:00.553949Z",
     "iopub.status.idle": "2025-04-13T03:18:00.560636Z",
     "shell.execute_reply": "2025-04-13T03:18:00.560004Z"
    },
    "papermill": {
     "duration": 0.027993,
     "end_time": "2025-04-13T03:18:00.562017",
     "exception": false,
     "start_time": "2025-04-13T03:18:00.534024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(\n",
    "    generator: nn.Module,\n",
    "    discriminator: nn.Module,\n",
    "    optim_g: optim.Optimizer,\n",
    "    optim_d: optim.Optimizer,\n",
    "    epoch: int,\n",
    "    *,\n",
    "    resolution: int, \n",
    "    alpha: float, \n",
    "    step: int, \n",
    "    fname: str = 'result.png', \n",
    "    rows: int = 4,\n",
    "    last: bool = True\n",
    ") -> None:\n",
    "    '''\n",
    "    Saves checkpoint and saves generated images from the checkpoint state.\n",
    "\n",
    "    Args:\n",
    "        generator (nn.Module): The generator network.\n",
    "        discriminator (nn.Module): The discriminator network.\n",
    "        optim_g (nn.Module): The generator's optimizer.\n",
    "        optim_d (nn.Module): The discriminator's optimizer.\n",
    "        epoch (int): The epoch of the checkpoint.\n",
    "        resolution (int): The current resolution of the network.\n",
    "        alpha (float): Fade-in alpha value.\n",
    "        step (int): The number of steps taken by the network.\n",
    "        fname (str, optional): The generated images file name. Default value is 'result.png'.\n",
    "        rows (int, optional): The number of rows of the generated images grid. Default value is 4.\n",
    "        last (bool, optional): Save as most recent checkpoint. Default value is True.\n",
    "    '''\n",
    "    \n",
    "    checkpoint = {\n",
    "        'generator': generator.state_dict(),\n",
    "        'discriminator': discriminator.state_dict(),\n",
    "        'optim_g': optim_g.state_dict(),\n",
    "        'optim_d': optim_d.state_dict(),\n",
    "        'resolution': resolution,\n",
    "        'epoch': epoch,\n",
    "        'alpha': alpha,\n",
    "        'step': step,\n",
    "    }\n",
    "    \n",
    "    epoch_path = os.path.join(f'epoch{epoch}')\n",
    "    os.makedirs(epoch_path, exist_ok=True)\n",
    "    torch.save(checkpoint, os.path.join(epoch_path, 'checkpoint.pt'))\n",
    "    \n",
    "    generator.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(rows * rows, LATENT_FEATURES, device=DEVICE)\n",
    "        outputs = generator(z, alpha, step)\n",
    "        outputs = F.interpolate(outputs, size=(RESOLUTION, RESOLUTION), mode='bilinear', align_corners=True)\n",
    "        \n",
    "        outputs = make_grid(outputs, nrow=rows, normalize=True)\n",
    "        save_image(outputs, os.path.join(epoch_path, fname))\n",
    "        \n",
    "        if last:\n",
    "            last_path = os.path.join('last')\n",
    "            os.makedirs(last_path, exist_ok=True)\n",
    "            torch.save(checkpoint, os.path.join(last_path, 'checkpoint.pt'))\n",
    "            save_image(outputs, os.path.join(last_path, fname))\n",
    "\n",
    "    generator.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b26af8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T03:18:00.599862Z",
     "iopub.status.busy": "2025-04-13T03:18:00.599629Z",
     "iopub.status.idle": "2025-04-13T03:18:00.603600Z",
     "shell.execute_reply": "2025-04-13T03:18:00.602944Z"
    },
    "papermill": {
     "duration": 0.02427,
     "end_time": "2025-04-13T03:18:00.604802",
     "exception": false,
     "start_time": "2025-04-13T03:18:00.580532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NonSaturatingLoss(nn.Module):\n",
    "\n",
    "    def d_loss(self, real_logits: torch.Tensor, fake_logits: torch.Tensor) -> torch.Tensor:\n",
    "        return F.softplus(-real_logits).mean() + F.softplus(fake_logits).mean()\n",
    "\n",
    "    def g_loss(self, fake_logits: torch.Tensor) -> torch.Tensor:\n",
    "        return F.softplus(-fake_logits).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311842e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T03:18:00.643323Z",
     "iopub.status.busy": "2025-04-13T03:18:00.643093Z",
     "iopub.status.idle": "2025-04-13T03:18:00.647568Z",
     "shell.execute_reply": "2025-04-13T03:18:00.646939Z"
    },
    "papermill": {
     "duration": 0.025315,
     "end_time": "2025-04-13T03:18:00.648776",
     "exception": false,
     "start_time": "2025-04-13T03:18:00.623461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# optim_g = optim.Adam([\n",
    "#     { 'params': [ p for name, p in generator.named_parameters() if not 'mapping' in name ] },\n",
    "#     { 'params': generator.mapping.parameters(), 'lr': LEARNING_RATE * 1e-2 }\n",
    "# ], lr=LEARNING_RATE, betas=(0, 0.99))\n",
    "optim_g = optim.Adam(generator.parameters(), lr=LEARNING_RATE, betas=(0, 0.99))\n",
    "optim_d = optim.Adam(discriminator.parameters(), lr=LEARNING_RATE, betas=(0, 0.99))\n",
    "criterion = NonSaturatingLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201431da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T03:18:00.686775Z",
     "iopub.status.busy": "2025-04-13T03:18:00.686529Z",
     "iopub.status.idle": "2025-04-13T03:18:00.698971Z",
     "shell.execute_reply": "2025-04-13T03:18:00.698349Z"
    },
    "papermill": {
     "duration": 0.032726,
     "end_time": "2025-04-13T03:18:00.700126",
     "exception": false,
     "start_time": "2025-04-13T03:18:00.667400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(checkpoint: str = None):\n",
    "    losses_d = []\n",
    "    losses_g = []\n",
    "\n",
    "    # Menentukan awal epoch, step dan alpha\n",
    "    start = 1\n",
    "    step = 0\n",
    "    alpha = 1e-5\n",
    "\n",
    "    # Mulai dari resolusi terkecil (biasanya 4x4)\n",
    "    current_resolution = 4\n",
    "\n",
    "    if checkpoint is not None and os.path.exists(checkpoint):\n",
    "        print('Resuming from last checkpoint...\\n')\n",
    "        last_checkpoint = torch.load(os.path.join(checkpoint), weights_only=True, map_location=DEVICE)\n",
    "        generator.load_state_dict(last_checkpoint['generator'])\n",
    "        discriminator.load_state_dict(last_checkpoint['discriminator'])\n",
    "        optim_d.load_state_dict(last_checkpoint['optim_d'])\n",
    "        optim_g.load_state_dict(last_checkpoint['optim_g'])\n",
    "        current_resolution = last_checkpoint['resolution']\n",
    "        start = last_checkpoint['epoch'] + 1\n",
    "        alpha = last_checkpoint['alpha']\n",
    "        step = last_checkpoint['step']\n",
    "    \n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "\n",
    "    # Dapatkan loader untuk resolusi awal\n",
    "    loader = get_loader(current_resolution)\n",
    "\n",
    "    # Parameter untuk peningkatan progresif\n",
    "    steps_per_resolution = len(loader) * (NUM_EPOCHS // (int(math.log2(RESOLUTION)) - 1))\n",
    "    alpha_step = 1.0 / steps_per_resolution\n",
    "    \n",
    "    for epoch in range(start, NUM_EPOCHS + 1):\n",
    "        print(f'[Epoch {epoch} / {NUM_EPOCHS}]')\n",
    "        avg_g_loss = 0\n",
    "        avg_d_loss = 0\n",
    "\n",
    "        # Periksa apakah harus menaikkan resolusi\n",
    "        if epoch % (NUM_EPOCHS // (int(math.log2(RESOLUTION)) - 1)) == 0 and current_resolution < RESOLUTION:\n",
    "            current_resolution *= 2\n",
    "            step += 1\n",
    "            alpha = 1e-5\n",
    "\n",
    "            # Atur ulang dataloader untuk resolusi baru\n",
    "            loader = get_loader(current_resolution)\n",
    "            steps_per_resolution = len(loader) * (NUM_EPOCHS // (int(math.log2(RESOLUTION)) - 1))\n",
    "            alpha_step = 1.0 / steps_per_resolution\n",
    "\n",
    "            print(f\"Resolution increased to {current_resolution}x{current_resolution}\")\n",
    "\n",
    "        progress_bar = tqdm(total=len(loader), \n",
    "                            desc=f'Train - Resolution: {current_resolution}x{current_resolution}', \n",
    "                            unit='step')\n",
    "        progress_bar.set_postfix_str(f'Alpha: {alpha:.4f}')\n",
    "\n",
    "        for real_img in loader:\n",
    "            b = real_img.size(0)\n",
    "            real_img = real_img.to(DEVICE)\n",
    "            \n",
    "            # Meningkatkan alpha secara bertahap\n",
    "            if alpha < 1.0:\n",
    "                alpha += alpha_step\n",
    "                alpha = min(alpha, 1)\n",
    "                progress_bar.set_postfix_str(f'Alpha: {alpha:.4f}')\n",
    "                \n",
    "            # Discriminator\n",
    "            for p in discriminator.parameters():\n",
    "                p.requires_grad = True\n",
    "                \n",
    "            optim_d.zero_grad()\n",
    "    \n",
    "            # Menghasilkan noise latent untuk StyleGAN\n",
    "            z = torch.randn(b, LATENT_FEATURES, device=DEVICE)\n",
    "    \n",
    "            # Menghasilkan gambar palsu dengan generator StyleGAN\n",
    "            fake_img = generator(z, alpha, step)\n",
    "    \n",
    "            # Forward pass pada discriminator dengan parameter progresif\n",
    "            real_logits = discriminator(real_img, alpha, step)\n",
    "            fake_logits = discriminator(fake_img.detach(), alpha, step)\n",
    "    \n",
    "            # R1 regularization\n",
    "            real_img.requires_grad_(True)\n",
    "            real_logits_reg = discriminator(real_img, alpha, step)\n",
    "            real_grad = torch.autograd.grad(outputs=real_logits_reg.sum(), inputs=real_img, create_graph=True)[0]\n",
    "            r1_reg = (R1_GAMMA / 2) * real_grad.pow(2).view(real_grad.shape[0], -1).sum(1).mean()\n",
    "            real_img.requires_grad_(False)\n",
    "    \n",
    "            # Total discriminator loss\n",
    "            d_loss = criterion.d_loss(real_logits, fake_logits) + r1_reg\n",
    "            avg_d_loss += d_loss.item()\n",
    "\n",
    "            d_loss.backward()\n",
    "            optim_d.step()\n",
    "\n",
    "            # Generator\n",
    "            for p in discriminator.parameters():\n",
    "                p.requires_grad = False\n",
    "                \n",
    "            optim_g.zero_grad()\n",
    "    \n",
    "            # Buat gambar palsu baru dan evaluasi\n",
    "            fake_img = generator(z, alpha, step)\n",
    "            fake_logits = discriminator(fake_img, alpha, step)\n",
    "            g_loss = criterion.g_loss(fake_logits)\n",
    "            avg_g_loss += g_loss.item()\n",
    "\n",
    "            g_loss.backward()\n",
    "            optim_g.step()\n",
    "                \n",
    "            progress_bar.update(1)\n",
    "    \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        avg_d_loss /= len(loader)\n",
    "        avg_g_loss /= len(loader)\n",
    "        print(f'Generator: {avg_g_loss:.4f}, Discriminator: {avg_d_loss:.4f}, Resolution: {current_resolution}x{current_resolution}, Alpha: {alpha:.4f}\\n')\n",
    "\n",
    "        losses_d.append(avg_d_loss)\n",
    "        losses_g.append(avg_g_loss)\n",
    "\n",
    "        if epoch % 5 != 0:\n",
    "            continue\n",
    "\n",
    "        save_checkpoint(\n",
    "            generator,\n",
    "            discriminator,\n",
    "            optim_g,\n",
    "            optim_d,\n",
    "            epoch=epoch,\n",
    "            resolution=current_resolution, \n",
    "            alpha=alpha, \n",
    "            step=step, \n",
    "        )\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses_g, label='Generator')\n",
    "    plt.plot(losses_d, label='Discriminator')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Losses')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('loss_plot.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31cf47b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T03:18:00.737724Z",
     "iopub.status.busy": "2025-04-13T03:18:00.737485Z",
     "iopub.status.idle": "2025-04-13T03:18:00.740623Z",
     "shell.execute_reply": "2025-04-13T03:18:00.739814Z"
    },
    "papermill": {
     "duration": 0.023264,
     "end_time": "2025-04-13T03:18:00.741938",
     "exception": false,
     "start_time": "2025-04-13T03:18:00.718674",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2500a5d2",
   "metadata": {
    "papermill": {
     "duration": 0.018127,
     "end_time": "2025-04-13T03:18:00.778417",
     "exception": false,
     "start_time": "2025-04-13T03:18:00.760290",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cff92f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T03:18:00.815920Z",
     "iopub.status.busy": "2025-04-13T03:18:00.815652Z",
     "iopub.status.idle": "2025-04-13T03:18:05.524435Z",
     "shell.execute_reply": "2025-04-13T03:18:05.523579Z"
    },
    "papermill": {
     "duration": 4.72904,
     "end_time": "2025-04-13T03:18:05.525715",
     "exception": false,
     "start_time": "2025-04-13T03:18:00.796675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load(os.path.join('last', 'checkpoint.pt'), weights_only=True, map_location=DEVICE)\n",
    "\n",
    "generator.load_state_dict(checkpoint['generator'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d608c771",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T03:18:05.565243Z",
     "iopub.status.busy": "2025-04-13T03:18:05.564957Z",
     "iopub.status.idle": "2025-04-13T03:18:13.709478Z",
     "shell.execute_reply": "2025-04-13T03:18:13.708625Z"
    },
    "papermill": {
     "duration": 8.179853,
     "end_time": "2025-04-13T03:18:13.724962",
     "exception": false,
     "start_time": "2025-04-13T03:18:05.545109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROWS = 8\n",
    "\n",
    "generator.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(ROWS * ROWS, LATENT_FEATURES, device=DEVICE)\n",
    "    imgs = generator(z)\n",
    "    \n",
    "    grid = make_grid(imgs, nrow=ROWS, normalize=True)\n",
    "    grid_np = grid.cpu().numpy().transpose((1, 2, 0))\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(grid_np)\n",
    "    plt.axis('off')\n",
    "    plt.title('Batik-StyleGAN Results')\n",
    "    plt.savefig('final_results.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e07b6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T03:18:13.807283Z",
     "iopub.status.busy": "2025-04-13T03:18:13.806924Z",
     "iopub.status.idle": "2025-04-13T03:18:19.715917Z",
     "shell.execute_reply": "2025-04-13T03:18:19.714886Z"
    },
    "papermill": {
     "duration": 5.950761,
     "end_time": "2025-04-13T03:18:19.717567",
     "exception": false,
     "start_time": "2025-04-13T03:18:13.766806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator.eval()\n",
    "\n",
    "inception_score = InceptionScore(normalize=True).to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(32, LATENT_FEATURES, device=DEVICE)\n",
    "    images = generator(z)\n",
    "    images = images * 0.5 + 0.5\n",
    "    images = F.interpolate(images, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "    inception_score.update(images)\n",
    "\n",
    "inception_mean, inception_std = inception_score.compute()\n",
    "\n",
    "print(f'IS: {inception_mean.item()} +/- {inception_std.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c059f419",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T03:18:19.797284Z",
     "iopub.status.busy": "2025-04-13T03:18:19.796919Z",
     "iopub.status.idle": "2025-04-13T03:19:52.827269Z",
     "shell.execute_reply": "2025-04-13T03:19:52.826114Z"
    },
    "papermill": {
     "duration": 93.110312,
     "end_time": "2025-04-13T03:19:52.867755",
     "exception": false,
     "start_time": "2025-04-13T03:18:19.757443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator.eval()\n",
    "\n",
    "loader = get_loader(RESOLUTION)\n",
    "fid = FrechetInceptionDistance(normalize=True).to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for real_img in loader:\n",
    "        real_img = real_img.to(DEVICE)\n",
    "        b = real_img.size(0)\n",
    "    \n",
    "        z = torch.randn(b, LATENT_FEATURES, device=DEVICE)\n",
    "        fake_img = generator(z)\n",
    "    \n",
    "        real_img = real_img * 0.5 + 0.5\n",
    "        fake_img = fake_img * 0.5 + 0.5\n",
    "    \n",
    "        real_img = F.interpolate(real_img, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "        fake_img = F.interpolate(fake_img, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "    \n",
    "        fid.update(real_img, real=True)\n",
    "        fid.update(fake_img, real=False)\n",
    "\n",
    "fid_score = fid.compute()\n",
    "\n",
    "print(f'FID: {fid_score.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67eea7c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T03:19:52.946017Z",
     "iopub.status.busy": "2025-04-13T03:19:52.945471Z",
     "iopub.status.idle": "2025-04-13T03:22:23.455155Z",
     "shell.execute_reply": "2025-04-13T03:22:23.454214Z"
    },
    "papermill": {
     "duration": 150.591457,
     "end_time": "2025-04-13T03:22:23.497551",
     "exception": false,
     "start_time": "2025-04-13T03:19:52.906094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PPLWrapper(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super(PPLWrapper, self).__init__()\n",
    "\n",
    "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            images = generator(z)\n",
    "        images = images * 0.5 + 0.5\n",
    "        images *= 255\n",
    "        images = images.to(torch.uint8)\n",
    "        return images\n",
    "\n",
    "    def sample(self, num_samples: int) -> torch.Tensor:\n",
    "        return torch.randn(num_samples, LATENT_FEATURES, device=DEVICE)\n",
    "\n",
    "generator.eval()\n",
    "\n",
    "ppl = PerceptualPathLength().to(DEVICE)\n",
    "\n",
    "ppl_mean, ppl_std, ppl_raw = ppl(PPLWrapper())\n",
    "\n",
    "print(f'PPL: {ppl_mean.item()} +/- {ppl_std.item()}, Raw: {ppl_raw}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54951d75",
   "metadata": {
    "papermill": {
     "duration": 0.040903,
     "end_time": "2025-04-13T03:22:23.579141",
     "exception": false,
     "start_time": "2025-04-13T03:22:23.538238",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8897c0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T03:22:23.661882Z",
     "iopub.status.busy": "2025-04-13T03:22:23.661545Z",
     "iopub.status.idle": "2025-04-13T03:22:25.795935Z",
     "shell.execute_reply": "2025-04-13T03:22:25.795180Z"
    },
    "papermill": {
     "duration": 2.17756,
     "end_time": "2025-04-13T03:22:25.797472",
     "exception": false,
     "start_time": "2025-04-13T03:22:23.619912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator.eval()\n",
    "\n",
    "z = torch.randn(1, LATENT_FEATURES, device=DEVICE)\n",
    "\n",
    "torch.onnx.export(\n",
    "    generator,\n",
    "    (z,),\n",
    "    'stylegan.onnx',\n",
    "    input_names=['z'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={'z': {0: 'batch_size'}},\n",
    "    opset_version=16,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6987767,
     "sourceId": 11193215,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 283457,
     "modelInstanceId": 280415,
     "sourceId": 334976,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 290.575163,
   "end_time": "2025-04-13T03:22:27.764694",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-13T03:17:37.189531",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
